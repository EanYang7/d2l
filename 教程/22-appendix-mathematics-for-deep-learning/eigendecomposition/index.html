
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="《动手学深度学习》">
      
      
        <meta name="author" content="Ean Yang">
      
      
        <link rel="canonical" href="https://eanyang7.github.io/d2l/%E6%95%99%E7%A8%8B/22-appendix-mathematics-for-deep-learning/eigendecomposition/">
      
      
        <link rel="prev" href="../distributions/">
      
      
        <link rel="next" href="../geometry-linear-algebraic-ops/">
      
      
      <link rel="icon" href="../../../assets/favicon.jpg">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.12">
    
    
      
        <title>Eigendecompositions - 动手学深度学习 Dive into Deep Learning#</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.fad675c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.356b1318.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-purple">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#eigendecompositions" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="动手学深度学习 Dive into Deep Learning#" class="md-header__button md-logo" aria-label="动手学深度学习 Dive into Deep Learning#" data-md-component="logo">
      
  <img src="../../../assets/logo.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            动手学深度学习 Dive into Deep Learning#
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Eigendecompositions
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-purple"  aria-label="切换为暗黑模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换为暗黑模式" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="red"  aria-label="切换为浅色模式"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="切换为浅色模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/EanYang7/d2l" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    github仓库
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  教程

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../%E7%BB%83%E4%B9%A0/" class="md-tabs__link">
          
  
  练习

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="动手学深度学习 Dive into Deep Learning#" class="md-nav__button md-logo" aria-label="动手学深度学习 Dive into Deep Learning#" data-md-component="logo">
      
  <img src="../../../assets/logo.jpg" alt="logo">

    </a>
    动手学深度学习 Dive into Deep Learning#
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/EanYang7/d2l" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    github仓库
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    前言
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../01-Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01-介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../_Installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    安装
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../_Notation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    符号
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../02-preliminaries/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    02 preliminaries
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../03-linear-regression/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    03 linear regression
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../04-linear-classification/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    04 linear classification
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../05-multilayer-perceptrons/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    05 multilayer perceptrons
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../06-builders-guide/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    06 builders guide
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../07-convolutional-modern/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    07 convolutional modern
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../08-convolutional-neural-networks/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    08 convolutional neural networks
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../09-recurrent-neural-networks/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    09 recurrent neural networks
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../10-recurrent-modern/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    10 recurrent modern
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../11-attention-mechanisms-and-transformers/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    11 attention mechanisms and transformers
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../12-optimization/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    12 optimization
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../13-computational-performance/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    13 computational performance
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../14-computer-vision/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    14 computer vision
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../15-natural-language-processing-pretraining/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    15 natural language processing pretraining
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../16-natural-language-processing-applications/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    16 natural language processing applications
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../17-reinforcement-learning/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    17 reinforcement learning
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../18-gaussian-processes/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    18 gaussian processes
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../19-hyperparameter-optimization/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    19 hyperparameter optimization
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../20-generative-adversarial-networks/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    20 generative adversarial networks
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../21-recommender-systems/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    21 recommender systems
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
    
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_25" checked>
        
          
          <label class="md-nav__link" for="__nav_2_25" id="__nav_2_25_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    22 appendix mathematics for deep learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_25_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_25">
            <span class="md-nav__icon md-icon"></span>
            22 appendix mathematics for deep learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Appendix: Mathematics for Deep Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../distributions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Eigendecompositions
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Eigendecompositions
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#finding-eigenvalues" class="md-nav__link">
    <span class="md-ellipsis">
      Finding Eigenvalues
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Finding Eigenvalues">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#an-example" class="md-nav__link">
    <span class="md-ellipsis">
      An Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#decomposing-matrices" class="md-nav__link">
    <span class="md-ellipsis">
      Decomposing Matrices
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#operations-on-eigendecompositions" class="md-nav__link">
    <span class="md-ellipsis">
      Operations on Eigendecompositions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eigendecompositions-of-symmetric-matrices" class="md-nav__link">
    <span class="md-ellipsis">
      Eigendecompositions of Symmetric Matrices
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gershgorin-circle-theorem" class="md-nav__link">
    <span class="md-ellipsis">
      Gershgorin Circle Theorem
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-useful-application-the-growth-of-iterated-maps" class="md-nav__link">
    <span class="md-ellipsis">
      A Useful Application: The Growth of Iterated Maps
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A Useful Application: The Growth of Iterated Maps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#eigenvectors-as-long-term-behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Eigenvectors as Long Term Behavior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#behavior-on-random-data" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior on Random Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relating-back-to-eigenvectors" class="md-nav__link">
    <span class="md-ellipsis">
      Relating Back to Eigenvectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#an-observation" class="md-nav__link">
    <span class="md-ellipsis">
      An Observation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fixing-the-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      Fixing the Normalization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discussion" class="md-nav__link">
    <span class="md-ellipsis">
      Discussion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exercises" class="md-nav__link">
    <span class="md-ellipsis">
      Exercises
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../geometry-linear-algebraic-ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Geometry and Linear Algebraic Operations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../information-theory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Information Theory
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../integral-calculus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Integral Calculus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../maximum-likelihood/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Maximum Likelihood
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../multivariable-calculus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multivariable Calculus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../naive-bayes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Naive Bayes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../random-variables/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Random Variables
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../single-variable-calculus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Single Variable Calculus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../statistics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistics
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../23-appendix-tools-for-deep-learning/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    23 appendix tools for deep learning
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../contrib/fasttext-pretraining/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Contrib
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    练习
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            练习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E7%BB%83%E4%B9%A0/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    动手学深度学习习题解答
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch02/ch02/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch02
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch03/ch03/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch03
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch04/ch04/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch04
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch05/ch05/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch05
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch06/ch06/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch06
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch07/ch07/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch07
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch08/ch08/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch08
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch09/ch09/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch09
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch10/ch10/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch10
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch11/ch11/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch11
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch12/ch12/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch12
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch13/ch13/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch13
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch14/ch14/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch14
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/ch15/ch15/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch15
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    
  
  
    <a href="../../../%E7%BB%83%E4%B9%A0/notebooks/ch02/ch02/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Notebooks
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#finding-eigenvalues" class="md-nav__link">
    <span class="md-ellipsis">
      Finding Eigenvalues
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Finding Eigenvalues">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#an-example" class="md-nav__link">
    <span class="md-ellipsis">
      An Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#decomposing-matrices" class="md-nav__link">
    <span class="md-ellipsis">
      Decomposing Matrices
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#operations-on-eigendecompositions" class="md-nav__link">
    <span class="md-ellipsis">
      Operations on Eigendecompositions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#eigendecompositions-of-symmetric-matrices" class="md-nav__link">
    <span class="md-ellipsis">
      Eigendecompositions of Symmetric Matrices
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gershgorin-circle-theorem" class="md-nav__link">
    <span class="md-ellipsis">
      Gershgorin Circle Theorem
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-useful-application-the-growth-of-iterated-maps" class="md-nav__link">
    <span class="md-ellipsis">
      A Useful Application: The Growth of Iterated Maps
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A Useful Application: The Growth of Iterated Maps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#eigenvectors-as-long-term-behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Eigenvectors as Long Term Behavior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#behavior-on-random-data" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior on Random Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relating-back-to-eigenvectors" class="md-nav__link">
    <span class="md-ellipsis">
      Relating Back to Eigenvectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#an-observation" class="md-nav__link">
    <span class="md-ellipsis">
      An Observation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fixing-the-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      Fixing the Normalization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discussion" class="md-nav__link">
    <span class="md-ellipsis">
      Discussion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exercises" class="md-nav__link">
    <span class="md-ellipsis">
      Exercises
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/EanYang7/d2l/tree/main/docs/教程/22-appendix-mathematics-for-deep-learning/eigendecomposition.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/EanYang7/d2l/tree/main/docs/教程/22-appendix-mathematics-for-deep-learning/eigendecomposition.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg>
    </a>
  


<h1 id="eigendecompositions">Eigendecompositions<a class="headerlink" href="#eigendecompositions" title="Permanent link">⚓︎</a></h1>
<p>:label:<code>sec_eigendecompositions</code></p>
<p>Eigenvalues are often one of the most useful notions
we will encounter when studying linear algebra,
however, as a beginner, it is easy to overlook their importance.
Below, we introduce eigendecomposition and
try to convey some sense of just why it is so important.</p>
<p>Suppose that we have a matrix <span class="arithmatex">\(A\)</span> with the following entries:</p>
<div class="arithmatex">\[
\mathbf{A} = \begin{bmatrix}
2 &amp; 0 \\
0 &amp; -1
\end{bmatrix}.
\]</div>
<p>If we apply <span class="arithmatex">\(A\)</span> to any vector <span class="arithmatex">\(\mathbf{v} = [x, y]^\top\)</span>,
we obtain a vector <span class="arithmatex">\(\mathbf{A}\mathbf{v} = [2x, -y]^\top\)</span>.
This has an intuitive interpretation:
stretch the vector to be twice as wide in the <span class="arithmatex">\(x\)</span>-direction,
and then flip it in the <span class="arithmatex">\(y\)</span>-direction.</p>
<p>However, there are <em>some</em> vectors for which something remains unchanged.
Namely <span class="arithmatex">\([1, 0]^\top\)</span> gets sent to <span class="arithmatex">\([2, 0]^\top\)</span>
and <span class="arithmatex">\([0, 1]^\top\)</span> gets sent to <span class="arithmatex">\([0, -1]^\top\)</span>.
These vectors are still in the same line,
and the only modification is that the matrix stretches them
by a factor of <span class="arithmatex">\(2\)</span> and <span class="arithmatex">\(-1\)</span> respectively.
We call such vectors <em>eigenvectors</em>
and the factor they are stretched by <em>eigenvalues</em>.</p>
<p>In general, if we can find a number <span class="arithmatex">\(\lambda\)</span>
and a vector <span class="arithmatex">\(\mathbf{v}\)</span> such that</p>
<div class="arithmatex">\[
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}.
\]</div>
<p>We say that <span class="arithmatex">\(\mathbf{v}\)</span> is an eigenvector for <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(\lambda\)</span> is an eigenvalue.</p>
<h2 id="finding-eigenvalues">Finding Eigenvalues<a class="headerlink" href="#finding-eigenvalues" title="Permanent link">⚓︎</a></h2>
<p>Let's figure out how to find them. By subtracting off the <span class="arithmatex">\(\lambda \mathbf{v}\)</span> from both sides,
and then factoring out the vector,
we see the above is equivalent to:</p>
<p><span class="arithmatex">\(<span class="arithmatex">\((\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = 0.\)</span>\)</span>
:eqlabel:<code>eq_eigvalue_der</code></p>
<p>For :eqref:<code>eq_eigvalue_der</code> to happen, we see that <span class="arithmatex">\((\mathbf{A} - \lambda \mathbf{I})\)</span>
must compress some direction down to zero,
hence it is not invertible, and thus the determinant is zero.
Thus, we can find the <em>eigenvalues</em>
by finding for what <span class="arithmatex">\(\lambda\)</span> is <span class="arithmatex">\(\det(\mathbf{A}-\lambda \mathbf{I}) = 0\)</span>.
Once we find the eigenvalues, we can solve
<span class="arithmatex">\(\mathbf{A}\mathbf{v} = \lambda \mathbf{v}\)</span>
to find the associated <em>eigenvector(s)</em>.</p>
<h3 id="an-example">An Example<a class="headerlink" href="#an-example" title="Permanent link">⚓︎</a></h3>
<p>Let's see this with a more challenging matrix</p>
<div class="arithmatex">\[
\mathbf{A} = \begin{bmatrix}
2 &amp; 1\\
2 &amp; 3
\end{bmatrix}.
\]</div>
<p>If we consider <span class="arithmatex">\(\det(\mathbf{A}-\lambda \mathbf{I}) = 0\)</span>,
we see this is equivalent to the polynomial equation
<span class="arithmatex">\(0 = (2-\lambda)(3-\lambda)-2 = (4-\lambda)(1-\lambda)\)</span>.
Thus, two eigenvalues are <span class="arithmatex">\(4\)</span> and <span class="arithmatex">\(1\)</span>.
To find the associated vectors, we then need to solve</p>
<div class="arithmatex">\[
\begin{bmatrix}
2 &amp; 1\\
2 &amp; 3
\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} = \begin{bmatrix}x \\ y\end{bmatrix}  \; \textrm{and} \;
\begin{bmatrix}
2 &amp; 1\\
2 &amp; 3
\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}  = \begin{bmatrix}4x \\ 4y\end{bmatrix} .
\]</div>
<p>We can solve this with the vectors <span class="arithmatex">\([1, -1]^\top\)</span> and <span class="arithmatex">\([1, 2]^\top\)</span> respectively.</p>
<p>We can check this in code using the built-in <code>numpy.linalg.eig</code> routine.</p>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab mxnet</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">mxnet</span> <span class="k">as</span> <span class="n">d2l</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]))</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab pytorch</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab tensorflow</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">d2l</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span>
</code></pre></div>
<p>Note that <code>numpy</code> normalizes the eigenvectors to be of length one,
whereas we took ours to be of arbitrary length.
Additionally, the choice of sign is arbitrary.
However, the vectors computed are parallel
to the ones we found by hand with the same eigenvalues.</p>
<h2 id="decomposing-matrices">Decomposing Matrices<a class="headerlink" href="#decomposing-matrices" title="Permanent link">⚓︎</a></h2>
<p>Let's continue the previous example one step further.  Let</p>
<div class="arithmatex">\[
\mathbf{W} = \begin{bmatrix}
1 &amp; 1 \\
-1 &amp; 2
\end{bmatrix},
\]</div>
<p>be the matrix where the columns are the eigenvectors of the matrix <span class="arithmatex">\(\mathbf{A}\)</span>. Let</p>
<div class="arithmatex">\[
\boldsymbol{\Sigma} = \begin{bmatrix}
1 &amp; 0 \\
0 &amp; 4
\end{bmatrix},
\]</div>
<p>be the matrix with the associated eigenvalues on the diagonal.
Then the definition of eigenvalues and eigenvectors tells us that</p>
<div class="arithmatex">\[
\mathbf{A}\mathbf{W} =\mathbf{W} \boldsymbol{\Sigma} .
\]</div>
<p>The matrix <span class="arithmatex">\(W\)</span> is invertible, so we may multiply both sides by <span class="arithmatex">\(W^{-1}\)</span> on the right,
we see that we may write</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\mathbf{A} = \mathbf{W} \boldsymbol{\Sigma} \mathbf{W}^{-1}.\)</span>\)</span>
:eqlabel:<code>eq_eig_decomp</code></p>
<p>In the next section we will see some nice consequences of this,
but for now we need only know that such a decomposition
will exist as long as we can find a full collection
of linearly independent eigenvectors (so that <span class="arithmatex">\(W\)</span> is invertible).</p>
<h2 id="operations-on-eigendecompositions">Operations on Eigendecompositions<a class="headerlink" href="#operations-on-eigendecompositions" title="Permanent link">⚓︎</a></h2>
<p>One nice thing about eigendecompositions :eqref:<code>eq_eig_decomp</code> is that
we can write many operations we usually encounter cleanly
in terms of the eigendecomposition. As a first example, consider:</p>
<div class="arithmatex">\[
\mathbf{A}^n = \overbrace{\mathbf{A}\cdots \mathbf{A}}^{\textrm{$n$ times}} = \overbrace{(\mathbf{W}\boldsymbol{\Sigma} \mathbf{W}^{-1})\cdots(\mathbf{W}\boldsymbol{\Sigma} \mathbf{W}^{-1})}^{\textrm{$n$ times}} =  \mathbf{W}\overbrace{\boldsymbol{\Sigma}\cdots\boldsymbol{\Sigma}}^{\textrm{$n$ times}}\mathbf{W}^{-1} = \mathbf{W}\boldsymbol{\Sigma}^n \mathbf{W}^{-1}.
\]</div>
<p>This tells us that for any positive power of a matrix,
the eigendecomposition is obtained by just raising the eigenvalues to the same power.
The same can be shown for negative powers,
so if we want to invert a matrix we need only consider</p>
<div class="arithmatex">\[
\mathbf{A}^{-1} = \mathbf{W}\boldsymbol{\Sigma}^{-1} \mathbf{W}^{-1},
\]</div>
<p>or in other words, just invert each eigenvalue.
This will work as long as each eigenvalue is non-zero,
so we see that invertible is the same as having no zero eigenvalues.</p>
<p>Indeed, additional work can show that if <span class="arithmatex">\(\lambda_1, \ldots, \lambda_n\)</span>
are the eigenvalues of a matrix, then the determinant of that matrix is</p>
<div class="arithmatex">\[
\det(\mathbf{A}) = \lambda_1 \cdots \lambda_n,
\]</div>
<p>or the product of all the eigenvalues.
This makes sense intuitively because whatever stretching <span class="arithmatex">\(\mathbf{W}\)</span> does,
<span class="arithmatex">\(W^{-1}\)</span> undoes it, so in the end the only stretching that happens is
by multiplication by the diagonal matrix <span class="arithmatex">\(\boldsymbol{\Sigma}\)</span>,
which stretches volumes by the product of the diagonal elements.</p>
<p>Finally, recall that the rank was the maximum number
of linearly independent columns of your matrix.
By examining the eigendecomposition closely,
we can see that the rank is the same
as the number of non-zero eigenvalues of <span class="arithmatex">\(\mathbf{A}\)</span>.</p>
<p>The examples could continue, but hopefully the point is clear:
eigendecomposition can simplify many linear-algebraic computations
and is a fundamental operation underlying many numerical algorithms
and much of the analysis that we do in linear algebra.</p>
<h2 id="eigendecompositions-of-symmetric-matrices">Eigendecompositions of Symmetric Matrices<a class="headerlink" href="#eigendecompositions-of-symmetric-matrices" title="Permanent link">⚓︎</a></h2>
<p>It is not always possible to find enough linearly independent eigenvectors
for the above process to work. For instance the matrix</p>
<div class="arithmatex">\[
\mathbf{A} = \begin{bmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{bmatrix},
\]</div>
<p>has only a single eigenvector, namely <span class="arithmatex">\((1, 0)^\top\)</span>.
To handle such matrices, we require more advanced techniques
than we can cover (such as the Jordan Normal Form, or Singular Value Decomposition).
We will often need to restrict our attention to those matrices
where we can guarantee the existence of a full set of eigenvectors.</p>
<p>The most commonly encountered family are the <em>symmetric matrices</em>,
which are those matrices where <span class="arithmatex">\(\mathbf{A} = \mathbf{A}^\top\)</span>.
In this case, we may take <span class="arithmatex">\(W\)</span> to be an <em>orthogonal matrix</em>—a matrix whose columns are all length one vectors that are at right angles to one another, where
<span class="arithmatex">\(\mathbf{W}^\top = \mathbf{W}^{-1}\)</span>—and all the eigenvalues will be real.
Thus, in this special case, we can write :eqref:<code>eq_eig_decomp</code> as</p>
<div class="arithmatex">\[
\mathbf{A} = \mathbf{W}\boldsymbol{\Sigma}\mathbf{W}^\top .
\]</div>
<h2 id="gershgorin-circle-theorem">Gershgorin Circle Theorem<a class="headerlink" href="#gershgorin-circle-theorem" title="Permanent link">⚓︎</a></h2>
<p>Eigenvalues are often difficult to reason with intuitively.
If presented an arbitrary matrix, there is little that can be said
about what the eigenvalues are without computing them.
There is, however, one theorem that can make it easy to approximate well
if the largest values are on the diagonal.</p>
<p>Let <span class="arithmatex">\(\mathbf{A} = (a_{ij})\)</span> be any square matrix (<span class="arithmatex">\(n\times n\)</span>).
We will define <span class="arithmatex">\(r_i = \sum_{j \neq i} |a_{ij}|\)</span>.
Let <span class="arithmatex">\(\mathcal{D}_i\)</span> represent the disc in the complex plane
with center <span class="arithmatex">\(a_{ii}\)</span> radius <span class="arithmatex">\(r_i\)</span>.
Then, every eigenvalue of <span class="arithmatex">\(\mathbf{A}\)</span> is contained in one of the <span class="arithmatex">\(\mathcal{D}_i\)</span>.</p>
<p>This can be a bit to unpack, so let's look at an example.
Consider the matrix:</p>
<div class="arithmatex">\[
\mathbf{A} = \begin{bmatrix}
1.0 &amp; 0.1 &amp; 0.1 &amp; 0.1 \\
0.1 &amp; 3.0 &amp; 0.2 &amp; 0.3 \\
0.1 &amp; 0.2 &amp; 5.0 &amp; 0.5 \\
0.1 &amp; 0.3 &amp; 0.5 &amp; 9.0
\end{bmatrix}.
\]</div>
<p>We have <span class="arithmatex">\(r_1 = 0.3\)</span>, <span class="arithmatex">\(r_2 = 0.6\)</span>, <span class="arithmatex">\(r_3 = 0.8\)</span> and <span class="arithmatex">\(r_4 = 0.9\)</span>.
The matrix is symmetric, so all eigenvalues are real.
This means that all of our eigenvalues will be in one of the ranges of</p>
<div class="arithmatex">\[[a_{11}-r_1, a_{11}+r_1] = [0.7, 1.3], \]</div>
<div class="arithmatex">\[[a_{22}-r_2, a_{22}+r_2] = [2.4, 3.6], \]</div>
<div class="arithmatex">\[[a_{33}-r_3, a_{33}+r_3] = [4.2, 5.8], \]</div>
<div class="arithmatex">\[[a_{44}-r_4, a_{44}+r_4] = [8.1, 9.9]. \]</div>
<p>Performing the numerical computation shows
that the eigenvalues are approximately <span class="arithmatex">\(0.99\)</span>, <span class="arithmatex">\(2.97\)</span>, <span class="arithmatex">\(4.95\)</span>, <span class="arithmatex">\(9.08\)</span>,
all comfortably inside the ranges provided.</p>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab mxnet</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]])</span>

<span class="n">v</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">v</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab pytorch</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]])</span>

<span class="n">v</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">v</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab tensorflow</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]])</span>

<span class="n">v</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">v</span>
</code></pre></div>
<p>In this way, eigenvalues can be approximated,
and the approximations will be fairly accurate
in the case that the diagonal is
significantly larger than all the other elements.</p>
<p>It is a small thing, but with a complex
and subtle topic like eigendecomposition,
it is good to get any intuitive grasp we can.</p>
<h2 id="a-useful-application-the-growth-of-iterated-maps">A Useful Application: The Growth of Iterated Maps<a class="headerlink" href="#a-useful-application-the-growth-of-iterated-maps" title="Permanent link">⚓︎</a></h2>
<p>Now that we understand what eigenvectors are in principle,
let's see how they can be used to provide a deep understanding
of a problem central to neural network behavior: proper weight initialization.</p>
<h3 id="eigenvectors-as-long-term-behavior">Eigenvectors as Long Term Behavior<a class="headerlink" href="#eigenvectors-as-long-term-behavior" title="Permanent link">⚓︎</a></h3>
<p>The full mathematical investigation of the initialization
of deep neural networks is beyond the scope of the text,
but we can see a toy version here to understand
how eigenvalues can help us see how these models work.
As we know, neural networks operate by interspersing layers
of linear transformations with non-linear operations.
For simplicity here, we will assume that there is no non-linearity,
and that the transformation is a single repeated matrix operation <span class="arithmatex">\(A\)</span>,
so that the output of our model is</p>
<div class="arithmatex">\[
\mathbf{v}_{out} = \mathbf{A}\cdot \mathbf{A}\cdots \mathbf{A} \mathbf{v}_{in} = \mathbf{A}^N \mathbf{v}_{in}.
\]</div>
<p>When these models are initialized, <span class="arithmatex">\(A\)</span> is taken to be
a random matrix with Gaussian entries, so let's make one of those.
To be concrete, we start with a mean zero, variance one Gaussian distributed <span class="arithmatex">\(5 \times 5\)</span> matrix.</p>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab mxnet</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">8675309</span><span class="p">)</span>

<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">A</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab pytorch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">A</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab tensorflow</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">A</span>
</code></pre></div>
<h3 id="behavior-on-random-data">Behavior on Random Data<a class="headerlink" href="#behavior-on-random-data" title="Permanent link">⚓︎</a></h3>
<p>For simplicity in our toy model,
we will assume that the data vector we feed in <span class="arithmatex">\(\mathbf{v}_{in}\)</span>
is a random five dimensional Gaussian vector.
Let's think about what we want to have happen.
For context, lets think of a generic ML problem,
where we are trying to turn input data, like an image, into a prediction,
like the probability the image is a picture of a cat.
If repeated application of <span class="arithmatex">\(\mathbf{A}\)</span>
stretches a random vector out to be very long,
then small changes in input will be amplified
into large changes in output---tiny modifications of the input image
would lead to vastly different predictions.
This does not seem right!</p>
<p>On the flip side, if <span class="arithmatex">\(\mathbf{A}\)</span> shrinks random vectors to be shorter,
then after running through many layers, the vector will essentially shrink to nothing,
and the output will not depend on the input. This is also clearly not right either!</p>
<p>We need to walk the narrow line between growth and decay
to make sure that our output changes depending on our input, but not much!</p>
<p>Let's see what happens when we repeatedly multiply our matrix <span class="arithmatex">\(\mathbf{A}\)</span>
against a random input vector, and keep track of the norm.</p>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab mxnet</span>
<span class="c1"># Calculate the sequence of norms after repeatedly applying `A`</span>
<span class="n">v_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">norm_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">v_in</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span>
    <span class="n">norm_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">))</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab pytorch</span>
<span class="c1"># Calculate the sequence of norms after repeatedly applying `A`</span>
<span class="n">v_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">norm_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">v_in</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">v_in</span>
    <span class="n">norm_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab tensorflow</span>
<span class="c1"># Calculate the sequence of norms after repeatedly applying `A`</span>
<span class="n">v_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">norm_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">v_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v_in</span><span class="p">)</span>
    <span class="n">norm_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">)</span>
</code></pre></div>
<p>The norm is growing uncontrollably!
Indeed if we take the list of quotients, we will see a pattern.</p>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab mxnet</span>
<span class="c1"># Compute the scaling factor of the norms</span>
<span class="n">norm_ratio_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">norm_ratio_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_ratio_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Ratio&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab pytorch</span>
<span class="c1"># Compute the scaling factor of the norms</span>
<span class="n">norm_ratio_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">norm_ratio_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_ratio_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Ratio&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab tensorflow</span>
<span class="c1"># Compute the scaling factor of the norms</span>
<span class="n">norm_ratio_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">norm_ratio_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_ratio_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Ratio&#39;</span><span class="p">)</span>
</code></pre></div>
<p>If we look at the last portion of the above computation,
we see that the random vector is stretched by a factor of <code>1.974459321485[...]</code>,
where the portion at the end shifts a little,
but the stretching factor is stable.</p>
<h3 id="relating-back-to-eigenvectors">Relating Back to Eigenvectors<a class="headerlink" href="#relating-back-to-eigenvectors" title="Permanent link">⚓︎</a></h3>
<p>We have seen that eigenvectors and eigenvalues correspond
to the amount something is stretched,
but that was for specific vectors, and specific stretches.
Let's take a look at what they are for <span class="arithmatex">\(\mathbf{A}\)</span>.
A bit of a caveat here: it turns out that to see them all,
we will need to go to complex numbers.
You can think of these as stretches and rotations.
By taking the norm of the complex number
(square root of the sums of squares of real and imaginary parts)
we can measure that stretching factor. Let's also sort them.</p>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab mxnet</span>
<span class="c1"># Compute the eigenvalues</span>
<span class="n">eigs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">norm_eigs</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">eigs</span><span class="p">]</span>
<span class="n">norm_eigs</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;norms of eigenvalues: </span><span class="si">{</span><span class="n">norm_eigs</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab pytorch</span>
<span class="c1"># Compute the eigenvalues</span>
<span class="n">eigs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">.</span><span class="n">eigenvalues</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">norm_eigs</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">eigs</span><span class="p">]</span>
<span class="n">norm_eigs</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;norms of eigenvalues: </span><span class="si">{</span><span class="n">norm_eigs</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab tensorflow</span>
<span class="c1"># Compute the eigenvalues</span>
<span class="n">eigs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">norm_eigs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">eigs</span><span class="p">]</span>
<span class="n">norm_eigs</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;norms of eigenvalues: </span><span class="si">{</span><span class="n">norm_eigs</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="an-observation">An Observation<a class="headerlink" href="#an-observation" title="Permanent link">⚓︎</a></h3>
<p>We see something a bit unexpected happening here:
that number we identified before for the
long term stretching of our matrix <span class="arithmatex">\(\mathbf{A}\)</span>
applied to a random vector is <em>exactly</em>
(accurate to thirteen decimal places!)
the largest eigenvalue of <span class="arithmatex">\(\mathbf{A}\)</span>.
This is clearly not a coincidence!</p>
<p>But, if we now think about what is happening geometrically,
this starts to make sense. Consider a random vector.
This random vector points a little in every direction,
so in particular, it points at least a little bit
in the same direction as the eigenvector of <span class="arithmatex">\(\mathbf{A}\)</span>
associated with the largest eigenvalue.
This is so important that it is called
the <em>principle eigenvalue</em> and <em>principle eigenvector</em>.
After applying <span class="arithmatex">\(\mathbf{A}\)</span>, our random vector
gets stretched in every possible direction,
as is associated with every possible eigenvector,
but it is stretched most of all in the direction
associated with this principle eigenvector.
What this means is that after apply in <span class="arithmatex">\(A\)</span>,
our random vector is longer, and points in a direction
closer to being aligned with the principle eigenvector.
After applying the matrix many times,
the alignment with the principle eigenvector becomes closer and closer until,
for all practical purposes, our random vector has been transformed
into the principle eigenvector!
Indeed this algorithm is the basis
for what is known as the <em>power iteration</em>
for finding the largest eigenvalue and eigenvector of a matrix. For details see, for example, :cite:<code>Golub.Van-Loan.1996</code>.</p>
<h3 id="fixing-the-normalization">Fixing the Normalization<a class="headerlink" href="#fixing-the-normalization" title="Permanent link">⚓︎</a></h3>
<p>Now, from above discussions, we concluded
that we do not want a random vector to be stretched or squished at all,
we would like random vectors to stay about the same size throughout the entire process.
To do so, we now rescale our matrix by this principle eigenvalue
so that the largest eigenvalue is instead now just one.
Let's see what happens in this case.</p>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab mxnet</span>
<span class="c1"># Rescale the matrix `A`</span>
<span class="n">A</span> <span class="o">/=</span> <span class="n">norm_eigs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Do the same experiment again</span>
<span class="n">v_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">norm_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">v_in</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span>
    <span class="n">norm_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">))</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab pytorch</span>
<span class="c1"># Rescale the matrix `A`</span>
<span class="n">A</span> <span class="o">/=</span> <span class="n">norm_eigs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Do the same experiment again</span>
<span class="n">v_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">norm_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">v_in</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">v_in</span>
    <span class="n">norm_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab tensorflow</span>
<span class="c1"># Rescale the matrix `A`</span>
<span class="n">A</span> <span class="o">/=</span> <span class="n">norm_eigs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Do the same experiment again</span>
<span class="n">v_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">norm_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">v_in</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">v_in</span><span class="p">)</span>
    <span class="n">norm_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_in</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">)</span>
</code></pre></div>
<p>We can also plot the ratio between consecutive norms as before and see that indeed it stabilizes.</p>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab mxnet</span>
<span class="c1"># Also plot the ratio</span>
<span class="n">norm_ratio_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">norm_ratio_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_ratio_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Ratio&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab pytorch</span>
<span class="c1"># Also plot the ratio</span>
<span class="n">norm_ratio_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">norm_ratio_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_ratio_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Ratio&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="input highlight"><pre><span></span><code><span class="c1">#@tab tensorflow</span>
<span class="c1"># Also plot the ratio</span>
<span class="n">norm_ratio_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">norm_ratio_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">/</span><span class="n">norm_list</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">norm_ratio_list</span><span class="p">,</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;Ratio&#39;</span><span class="p">)</span>
</code></pre></div>
<h2 id="discussion">Discussion<a class="headerlink" href="#discussion" title="Permanent link">⚓︎</a></h2>
<p>We now see exactly what we hoped for!
After normalizing the matrices by the principal eigenvalue,
we see that the random data does not explode as before,
but rather eventually equilibrates to a specific value.
It would be nice to be able to do these things from first principles,
and it turns out that if we look deeply at the mathematics of it,
we can see that the largest eigenvalue
of a large random matrix with independent mean zero,
variance one Gaussian entries is on average about <span class="arithmatex">\(\sqrt{n}\)</span>,
or in our case <span class="arithmatex">\(\sqrt{5} \approx 2.2\)</span>,
due to a fascinating fact known as the <em>circular law</em> :cite:<code>Ginibre.1965</code>.
The relationship between the eigenvalues (and a related object called singular values) of random matrices has been shown to have deep connections to proper initialization of neural networks as was discussed in :citet:<code>Pennington.Schoenholz.Ganguli.2017</code> and subsequent works.</p>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">⚓︎</a></h2>
<ul>
<li>Eigenvectors are vectors which are stretched by a matrix without changing direction.</li>
<li>Eigenvalues are the amount that the eigenvectors are stretched by the application of the matrix.</li>
<li>The eigendecomposition of a matrix can allow for many operations to be reduced to operations on the eigenvalues.</li>
<li>The Gershgorin Circle Theorem can provide approximate values for the eigenvalues of a matrix.</li>
<li>The behavior of iterated matrix powers depends primarily on the size of the largest eigenvalue.  This understanding has many applications in the theory of neural network initialization.</li>
</ul>
<h2 id="exercises">Exercises<a class="headerlink" href="#exercises" title="Permanent link">⚓︎</a></h2>
<ol>
<li>What are the eigenvalues and eigenvectors of
$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 1 \
1 &amp; 2
\end{bmatrix}?
$$</li>
<li>What are the eigenvalues and eigenvectors of the following matrix, and what is strange about this example compared to the previous one?
$$
\mathbf{A} = \begin{bmatrix}
2 &amp; 1 \
0 &amp; 2
\end{bmatrix}.
$$</li>
<li>Without computing the eigenvalues, is it possible that the smallest eigenvalue of the following matrix is less that <span class="arithmatex">\(0.5\)</span>? <em>Note</em>: this problem can be done in your head.
$$
\mathbf{A} = \begin{bmatrix}
3.0 &amp; 0.1 &amp; 0.3 &amp; 1.0 \
0.1 &amp; 1.0 &amp; 0.1 &amp; 0.2 \
0.3 &amp; 0.1 &amp; 5.0 &amp; 0.0 \
1.0 &amp; 0.2 &amp; 0.0 &amp; 1.8
\end{bmatrix}.
$$</li>
</ol>
<p>:begin_tab:<code>mxnet</code>
<a href="https://discuss.d2l.ai/t/411">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>pytorch</code>
<a href="https://discuss.d2l.ai/t/1086">Discussions</a>
:end_tab:</p>
<p>:begin_tab:<code>tensorflow</code>
<a href="https://discuss.d2l.ai/t/1087">Discussions</a>
:end_tab:</p>

  <hr>
<div class="md-source-file">
  <small>
    
      最后更新:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 25, 2023</span>
      
        <br>
        创建日期:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 25, 2023</span>
      
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../distributions/" class="md-footer__link md-footer__link--prev" aria-label="上一页: Distributions">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                Distributions
              </div>
            </div>
          </a>
        
        
          
          <a href="../geometry-linear-algebraic-ops/" class="md-footer__link md-footer__link--next" aria-label="下一页: Geometry and Linear Algebraic Operations">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                Geometry and Linear Algebraic Operations
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 Ean Yang
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/YQisme" target="_blank" rel="noopener" title="github主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://space.bilibili.com/244185393?spm_id_from=333.788.0.0" target="_blank" rel="noopener" title="b站主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M488.6 104.1c16.7 18.1 24.4 39.7 23.3 65.7v202.4c-.4 26.4-9.2 48.1-26.5 65.1-17.2 17-39.1 25.9-65.5 26.7H92.02c-26.45-.8-48.21-9.8-65.28-27.2C9.682 419.4.767 396.5 0 368.2V169.8c.767-26 9.682-47.6 26.74-65.7C43.81 87.75 65.57 78.77 92.02 78h29.38L96.05 52.19c-5.75-5.73-8.63-13-8.63-21.79 0-8.8 2.88-16.06 8.63-21.797C101.8 2.868 109.1 0 117.9 0s16.1 2.868 21.9 8.603L213.1 78h88l74.5-69.397C381.7 2.868 389.2 0 398 0c8.8 0 16.1 2.868 21.9 8.603 5.7 5.737 8.6 12.997 8.6 21.797 0 8.79-2.9 16.06-8.6 21.79L394.6 78h29.3c26.4.77 48 9.75 64.7 26.1zm-38.8 69.7c-.4-9.6-3.7-17.4-10.7-23.5-5.2-6.1-14-9.4-22.7-9.8H96.05c-9.59.4-17.45 3.7-23.58 9.8-6.14 6.1-9.4 13.9-9.78 23.5v194.4c0 9.2 3.26 17 9.78 23.5s14.38 9.8 23.58 9.8H416.4c9.2 0 17-3.3 23.3-9.8 6.3-6.5 9.7-14.3 10.1-23.5V173.8zm-264.3 42.7c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.2 6.3-14 9.5-23.6 9.5-9.6 0-17.5-3.2-23.6-9.5-6.1-6.3-9.4-14-9.8-23.2v-33.3c.4-9.1 3.8-16.9 10.1-23.2 6.3-6.3 13.2-9.6 23.3-10 9.2.4 17 3.7 23.3 10zm191.5 0c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.1 6.3-14 9.5-23.6 9.5-9.6 0-17.4-3.2-23.6-9.5-7-6.3-9.4-14-9.7-23.2v-33.3c.3-9.1 3.7-16.9 10-23.2 6.3-6.3 14.1-9.6 23.3-10 9.2.4 17 3.7 23.3 10z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://eanyang7.com" target="_blank" rel="noopener" title="个人主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M112 48a48 48 0 1 1 96 0 48 48 0 1 1-96 0zm40 304v128c0 17.7-14.3 32-32 32s-32-14.3-32-32V256.9l-28.6 47.6c-9.1 15.1-28.8 20-43.9 10.9s-20-28.8-10.9-43.9l58.3-97c17.4-28.9 48.6-46.6 82.3-46.6h29.7c33.7 0 64.9 17.7 82.3 46.6l58.3 97c9.1 15.1 4.2 34.8-10.9 43.9s-34.8 4.2-43.9-10.9L232 256.9V480c0 17.7-14.3 32-32 32s-32-14.3-32-32V352h-16z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.instant", "navigation.instant.progress", "navigation.tracking", "navigation.tabs", "navigation.prune", "navigation.top", "toc.follow", "header.autohide", "navigation.footer", "search.suggest", "search.highlight", "search.share", "content.action.edit", "content.action.view", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.6c14ae12.min.js"></script>
      
    
  </body>
</html>