
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="《动手学深度学习》">
      
      
        <meta name="author" content="Ean Yang">
      
      
        <link rel="canonical" href="https://eanyang7.github.io/d2l/%E7%BB%83%E4%B9%A0/ch11/ch11/">
      
      
        <link rel="prev" href="../../ch10/ch10/">
      
      
        <link rel="next" href="../../ch12/ch12/">
      
      
      <link rel="icon" href="../../../assets/favicon.jpg">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.12">
    
    
      
        <title>第11章 优化算法 - 动手学深度学习 Dive into Deep Learning#</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.fad675c6.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.356b1318.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-purple">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#11" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="动手学深度学习 Dive into Deep Learning#" class="md-header__button md-logo" aria-label="动手学深度学习 Dive into Deep Learning#" data-md-component="logo">
      
  <img src="../../../assets/logo.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            动手学深度学习 Dive into Deep Learning#
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              第11章 优化算法
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-purple"  aria-label="切换为暗黑模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换为暗黑模式" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="red"  aria-label="切换为浅色模式"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="切换为浅色模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/EanYang7/d2l" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    github仓库
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../%E6%95%99%E7%A8%8B/" class="md-tabs__link">
          
  
  教程

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  练习

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="动手学深度学习 Dive into Deep Learning#" class="md-nav__button md-logo" aria-label="动手学深度学习 Dive into Deep Learning#" data-md-component="logo">
      
  <img src="../../../assets/logo.jpg" alt="logo">

    </a>
    动手学深度学习 Dive into Deep Learning#
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/EanYang7/d2l" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    github仓库
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    教程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%95%99%E7%A8%8B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    前言
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%95%99%E7%A8%8B/01-Introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    01-介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%95%99%E7%A8%8B/_Installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    安装
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E6%95%99%E7%A8%8B/_Notation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    符号
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/02-preliminaries/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    02 preliminaries
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/03-linear-regression/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    03 linear regression
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/04-linear-classification/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    04 linear classification
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/05-multilayer-perceptrons/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    05 multilayer perceptrons
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/06-builders-guide/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    06 builders guide
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/07-convolutional-modern/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    07 convolutional modern
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/08-convolutional-neural-networks/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    08 convolutional neural networks
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/09-recurrent-neural-networks/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    09 recurrent neural networks
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/10-recurrent-modern/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    10 recurrent modern
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/11-attention-mechanisms-and-transformers/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    11 attention mechanisms and transformers
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/12-optimization/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    12 optimization
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/13-computational-performance/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    13 computational performance
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/14-computer-vision/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    14 computer vision
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/15-natural-language-processing-pretraining/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    15 natural language processing pretraining
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/16-natural-language-processing-applications/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    16 natural language processing applications
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/17-reinforcement-learning/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    17 reinforcement learning
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/18-gaussian-processes/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    18 gaussian processes
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/19-hyperparameter-optimization/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    19 hyperparameter optimization
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/20-generative-adversarial-networks/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    20 generative adversarial networks
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/21-recommender-systems/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    21 recommender systems
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/22-appendix-mathematics-for-deep-learning/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    22 appendix mathematics for deep learning
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/23-appendix-tools-for-deep-learning/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    23 appendix tools for deep learning
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../%E6%95%99%E7%A8%8B/contrib/fasttext-pretraining/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Contrib
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    练习
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            练习
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    动手学深度学习习题解答
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch02/ch02/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch02
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch03/ch03/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch03
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch04/ch04/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch04
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch05/ch05/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch05
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch06/ch06/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch06
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch07/ch07/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch07
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch08/ch08/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch08
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch09/ch09/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch09
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch10/ch10/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch10
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
    
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_11" checked>
        
          
          <label class="md-nav__link" for="__nav_3_11" id="__nav_3_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Ch11
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_11_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_11">
            <span class="md-nav__icon md-icon"></span>
            Ch11
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    第11章 优化算法
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    第11章 优化算法
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 优化与深度学习
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.1 优化与深度学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1111" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.1.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1112" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.1.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1113" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.1.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1114" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.1.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#112" class="md-nav__link">
    <span class="md-ellipsis">
      11.2 凸性
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.2 凸性">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1121" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1122" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1123" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1124" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1125" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.5
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1126" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.6
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1127" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.7
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1128" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.8
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#113" class="md-nav__link">
    <span class="md-ellipsis">
      11.3 梯度下降
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.3 梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1131" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.3.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1132" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.3.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1133" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.3.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1134" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.3.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#114" class="md-nav__link">
    <span class="md-ellipsis">
      11.4 随机梯度下降
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.4 随机梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1141" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1142" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1143" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1144" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1145" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.5
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#115" class="md-nav__link">
    <span class="md-ellipsis">
      11.5 小批量随机梯度下降
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.5 小批量随机梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1151" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.5.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1152" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.5.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1153" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.5.3
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#116" class="md-nav__link">
    <span class="md-ellipsis">
      11.6 动量法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.6 动量法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1161" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.6.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1162" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.6.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1163" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.6.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1164" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.6.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#117-adagrad" class="md-nav__link">
    <span class="md-ellipsis">
      11.7 AdaGrad算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.7 AdaGrad算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1171" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1172" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1173" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1174" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1175" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.5
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1176" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.6
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#118-rmsprop" class="md-nav__link">
    <span class="md-ellipsis">
      11.8 RMSProp算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.8 RMSProp算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1181" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.8.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1182" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.8.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1183" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.8.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1184" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.8.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#119-adadelta" class="md-nav__link">
    <span class="md-ellipsis">
      11.9 Adadelta
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.9 Adadelta">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1191" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.9.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1192" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.9.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1193" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.9.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1194" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.9.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1110-adam" class="md-nav__link">
    <span class="md-ellipsis">
      11.10 Adam算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.10 Adam算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11101" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.10.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11102" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.10.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11103" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.10.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11104" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.10.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1111_1" class="md-nav__link">
    <span class="md-ellipsis">
      11.11 学习率调度器
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.11 学习率调度器">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11111" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11112" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11113" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11114" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11115" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.5
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch12/ch12/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch12
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch13/ch13/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch13
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch14/ch14/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch14
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../ch15/ch15/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Ch15
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
    
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    
  
  
    <a href="../../notebooks/ch02/ch02/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Notebooks
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 优化与深度学习
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.1 优化与深度学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1111" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.1.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1112" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.1.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1113" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.1.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1114" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.1.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#112" class="md-nav__link">
    <span class="md-ellipsis">
      11.2 凸性
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.2 凸性">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1121" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1122" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1123" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1124" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1125" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.5
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1126" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.6
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1127" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.7
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1128" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.2.8
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#113" class="md-nav__link">
    <span class="md-ellipsis">
      11.3 梯度下降
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.3 梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1131" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.3.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1132" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.3.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1133" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.3.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1134" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.3.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#114" class="md-nav__link">
    <span class="md-ellipsis">
      11.4 随机梯度下降
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.4 随机梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1141" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1142" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1143" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1144" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1145" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.4.5
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#115" class="md-nav__link">
    <span class="md-ellipsis">
      11.5 小批量随机梯度下降
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.5 小批量随机梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1151" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.5.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1152" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.5.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1153" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.5.3
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#116" class="md-nav__link">
    <span class="md-ellipsis">
      11.6 动量法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.6 动量法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1161" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.6.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1162" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.6.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1163" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.6.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1164" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.6.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#117-adagrad" class="md-nav__link">
    <span class="md-ellipsis">
      11.7 AdaGrad算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.7 AdaGrad算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1171" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1172" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1173" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1174" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1175" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.5
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1176" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.7.6
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#118-rmsprop" class="md-nav__link">
    <span class="md-ellipsis">
      11.8 RMSProp算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.8 RMSProp算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1181" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.8.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1182" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.8.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1183" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.8.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1184" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.8.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#119-adadelta" class="md-nav__link">
    <span class="md-ellipsis">
      11.9 Adadelta
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.9 Adadelta">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1191" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.9.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1192" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.9.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1193" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.9.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1194" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.9.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1110-adam" class="md-nav__link">
    <span class="md-ellipsis">
      11.10 Adam算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.10 Adam算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11101" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.10.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11102" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.10.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11103" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.10.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11104" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.10.4
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1111_1" class="md-nav__link">
    <span class="md-ellipsis">
      11.11 学习率调度器
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11.11 学习率调度器">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11111" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11112" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11113" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.3
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11114" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.4
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#11115" class="md-nav__link">
    <span class="md-ellipsis">
      练习11.11.5
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/EanYang7/d2l/tree/main/docs/练习/ch11/ch11.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/EanYang7/d2l/tree/main/docs/练习/ch11/ch11.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg>
    </a>
  


<h1 id="11">第11章 优化算法<a class="headerlink" href="#11" title="Permanent link">⚓︎</a></h1>
<h2 id="111">11.1 优化与深度学习<a class="headerlink" href="#111" title="Permanent link">⚓︎</a></h2>
<h3 id="1111">练习11.1.1<a class="headerlink" href="#1111" title="Permanent link">⚓︎</a></h3>
<ol>
<li>考虑一个简单的MLP，它有一个隐藏层，比如，隐藏层中维度为d
和一个输出。证明对于任何局部最小值，至少有d！
个等效方案。</li>
</ol>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">out</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">output</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>tensor([-0.0249], grad_fn=&lt;AddBackward0&gt;)
</code></pre></div>
<p>我觉得这个问题可以转换一个数学的排列组合问题。</p>
<ul>
<li>核心思想：
证明对于任何局部最小值，至少有d!个等效方案。我们可以通过重新排列隐藏层中的权重和偏置来得到等效的局部最小值。</li>
</ul>
<p>证明步骤：
1. 定义MLP的参数，包括隐藏层的权重矩阵W和偏置向量b，以及输出层的权重向量w和偏置b'。
2. 假设存在一个局部最小值L，对应的参数为(W, b, w, b')。
3. 针对隐藏层的权重矩阵W，我们知道隐藏层中有d个神经元，每个神经元有一个权重向量。我们可以将这些权重向量按照不同的顺序排列，得到不同的权重矩阵W'。
4. 同样地，我们可以对隐藏层的偏置向量b进行重排，得到不同的偏置向量b'。
5. 将原始的参数(W, b, w, b')转换为新的参数(W', b', w, b')。
6. 通过将隐藏层的权重矩阵W替换为W'，偏置向量b替换为b'，得到一个新的局部最小值L'。
7. 证明L'是一个局部最小值，因为我们只是对隐藏层的参数进行了重排，而没有改变神经元之间的连接关系或激活函数。因此，对于相同的输入，输出也是相同的。
8. 由于我们可以通过重新排列隐藏层的权重和偏置来得到等效的局部最小值，而隐藏层中有d个神经元，每个神经元有d!种排列方式，因此至少存在d!个等效方案。</p>
<p>通过以上步骤，我们证明了对于任何局部最小值，至少有d!个等效方案。</p>
<h3 id="1112">练习11.1.2<a class="headerlink" href="#1112" title="Permanent link">⚓︎</a></h3>
<p>2.假设我们有一个对称随机矩阵M，其中条目<span class="arithmatex">\(M_{ij}=M_{ji}\)</span>
各自从某种概率分布<span class="arithmatex">\(P_{ij}\)</span>中抽取。此外，假设<span class="arithmatex">\(p_{ij}=p_{ji}\)</span>，即分布是对称的（详情请参见 (Wigner, 1958)）。</p>
<ol>
<li>
<p>证明特征值的分布也是对称的。也就是说，对于任何特征向量<span class="arithmatex">\(\lambda\)</span>，关联的特征值
满足<span class="arithmatex">\(P(\lambda &gt; 0)=P(\lambda &lt; 0)\)</span>的概率为<span class="arithmatex">\(P(\lambda &gt; 0)=P(\lambda &lt; 0)\)</span>。</p>
</li>
<li>
<p>为什么以上没有暗示<span class="arithmatex">\(P(\lambda &gt; 0)=0.5\)</span></p>
</li>
</ol>
<p><strong>解答：</strong></p>
<p>1.要证明特征值的分布也是对称的，我们可以使用性质：对于任意对称矩阵M，其特征值都是实数。</p>
<p>假设<span class="arithmatex">\(\lambda\)</span>是M的一个特征值，对应的特征向量为v。由于M是对称矩阵，我们有：</p>
<div class="arithmatex">\[Mv = \lambda v\]</div>
<p>对上式两边同时取共轭转置，得到：</p>
<div class="arithmatex">\[M^*v^* = \lambda^* v^*\]</div>
<p>其中<span class="arithmatex">\(M^*\)</span>表示M的共轭转置。由于M是实数矩阵，所以<span class="arithmatex">\(M^* = M\)</span>，于是上式可以写为：</p>
<div class="arithmatex">\[Mv^* = \lambda^* v^*\]</div>
<p>这意味着<span class="arithmatex">\(\lambda^*\)</span>也是M的特征值，对应的特征向量为<span class="arithmatex">\(v^*\)</span>。由于M的特征值都是实数，所以<span class="arithmatex">\(\lambda = \lambda^*\)</span>，即特征值是对称的。</p>
<p>2.上述结论并不暗示<span class="arithmatex">\(P(\lambda &gt; 0) = 0.5\)</span>。特征值的对称性只是说明了特征值分布关于原点对称，即<span class="arithmatex">\(P(\lambda &gt; 0) = P(\lambda &lt; 0)\)</span>。但是，并没有暗示特征值分布在正值和负值上的概率是相等的。特征值的具体分布取决于概率分布<span class="arithmatex">\(P_{ij}\)</span>的具体形式，无法直接得出<span class="arithmatex">\(P(\lambda &gt; 0) = 0.5\)</span>的结论。</p>
<h3 id="1113">练习11.1.3<a class="headerlink" href="#1113" title="Permanent link">⚓︎</a></h3>
<p>3.你能想到深度学习优化还涉及哪些其他挑战？</p>
<p><strong>解答：</strong></p>
<p>梯度消失，梯度爆炸，局部最优解，鞍点，loss不收敛，梯度悬崖...</p>
<h3 id="1114">练习11.1.4<a class="headerlink" href="#1114" title="Permanent link">⚓︎</a></h3>
<p>4.假设你想在（真实的）鞍上平衡一个（真实的）球。</p>
<ol>
<li>
<p>为什么这很难？</p>
</li>
<li>
<p>能利用这种效应来优化算法吗？</p>
</li>
</ol>
<p><strong>解答：</strong></p>
<p>(1)为什么这么难？
1. 考虑球和鞍的几何形状和性质。鞍是一个具有曲率和变化的表面，而球是一个固体物体。</p>
<ol>
<li>
<p>球在鞍上的平衡涉及到重力、重心位置以及鞍的形状对球的支撑和稳定性的影响。
由于鞍的形状，球在鞍上的平衡点通常是一个不稳定的平衡点，稍微有一点扰动就可能使球失去平衡。
球的重心位置很容易偏离鞍的平衡点，这会导致球在鞍上的平衡非常困难。</p>
</li>
<li>
<p>将一个球平衡在一个鞍上的困难可以归因于一个物理效应，即不稳定性效应。</p>
</li>
</ol>
<p>不稳定性效应是指系统在某个平衡点附近的微小扰动会引起系统远离平衡点的现象。在这个问题中，鞍是一个不稳定的平衡点，即球在鞍的顶点上的平衡点是不稳定的。这意味着，即使球在鞍的顶点上保持静止，微小的扰动或者偏移都会导致球失去平衡，滚落到鞍的一侧。</p>
<p>这种不稳定性效应是由鞍的形状和球的重心位置的限制所导致的。由于鞍的形状，球在鞍上的平衡点非常狭窄，稍微有一点扰动就可能使球失去平衡。同时，球的重心位置很容易偏离鞍的平衡点，由于重力的作用，球的重心位置通常会偏向鞍的一侧，而不是位于鞍的顶点上。这使得球在鞍上的平衡非常困难。</p>
<p>因此，将一个球平衡在一个鞍上的困难可以归因于不稳定性效应，即微小的扰动会导致球失去平衡，滚落到鞍的一侧。</p>
<p>(2)能利用这种效应来优化算法吗？</p>
<p>不稳定性在深度学习的优化种，即随机性和扰动性。
- 利用随机型：网络层的Dropout随机丢弃神经元；神经元权重随机初始化；
- 利用扰动性：网络加入噪声；mask机制；VAE中的denoise；diffuison
- 模拟退火，遗传算法</p>
<h2 id="112">11.2 凸性<a class="headerlink" href="#112" title="Permanent link">⚓︎</a></h2>
<h3 id="1121">练习11.2.1<a class="headerlink" href="#1121" title="Permanent link">⚓︎</a></h3>
<p>1.假设我们想要通过绘制集合内点之间的所有直线并检查这些直线是否包含来验证集合的凸性。</p>
<p>i.证明只检查边界上的点是充分的。</p>
<p>ii.证明只检查集合的顶点是充分的。</p>
<p><strong>解答：</strong></p>
<p>i.假设集合内存在两个不在边界上的点A和B。我们需要证明点A和点B之间的连线会跨越边界，进入集合外部。
1. 假设点A和点B之间的连线与边界相交于点C。
2. 由于点A和点B不在边界上，存在一条从点C出发的射线，它与边界只在点C相交，并且在点C以外的部分都与边界不相交。
3. 考虑射线上的一个点D，位于点C以外，但在集合内部。
4. 点D与点A之间的连线在点B的一侧与边界相交，这意味着点B也不在边界上。
5. 由此可以得出，如果集合内存在两个不在边界上的点A和B，那么至少存在一条由这两个点组成的线段与边界交叉。这与集合的凸性定义相矛盾，因此我们可以推断，只要边界上的点满足凸性要求，整个集合就是凸的。</p>
<p>ii.假设我们检查了集合的所有顶点，并且这些点之间的连线都完全位于集合内部。
1. 假设集合的顶点A和顶点B之间存在一条线段，它与边界相交于点C。
2. 由于集合的顶点是凸集中的极值点，不存在其他点可以在点C周围找到。
3. 考虑线段上的一个内部点D，它位于点C之外但在集合内部。
4. 点D与点A之间的连线在点B的一侧与边界相交，这意味着点B也是集合的顶点之一。
5. 根据凸集的定义，集合中的任意两点之间的连线都完全位于集合内部，因此点A和点B之间的连线也应该完全位于集合内部。由此可以得出，如果我们检查了集合的所有顶点，并且这些点之间的连线都满足凸性要求，那么整个集合就是凸的。</p>
<h3 id="1122">练习11.2.2<a class="headerlink" href="#1122" title="Permanent link">⚓︎</a></h3>
<p>2.用<span class="arithmatex">\(p\)</span>-范数表示半径为<span class="arithmatex">\(r\)</span>的球，证明<span class="arithmatex">\(\mathcal{B}_p[r] := \{\mathbf{x} | \mathbf{x} \in \mathbb{R}^d \text{ and } \|\mathbf{x}\|_p \leq r\}\)</span>，<span class="arithmatex">\(\mathcal{B}_p[r]\)</span>对于所有<span class="arithmatex">\(p \geq 1\)</span>是凸的。</p>
<p><strong>解答：</strong></p>
<p>在这个问题中，我们需要证明，对于所有的p范数，半径为r的球的凸集性。p-范数形式的说明如下：<span class="arithmatex">\(\|x\|_p = (\sum \limits_{i=1}^n |x_i|^p)^{\frac{1}{p}}\)</span>，其中<span class="arithmatex">\(x \in \mathbb{R}^n\)</span>是一个n维向量，<span class="arithmatex">\(p\geq 1\)</span>。</p>
<p>要证明一个集合是凸（convex）的，我们需要满足以下条件：对于该集合中的任意两点，将这两点连接起来形成的线段也在该集合中。</p>
<p>对于定义的集合<span class="arithmatex">\(\mathcal{B}_p[r]\)</span>，如果<span class="arithmatex">\(x,y \in \mathcal{B}_p[r]\)</span>，我们需要证明在
<span class="arithmatex">\(0 \leq t \leq 1\)</span>的任意情况下，<span class="arithmatex">\(tx + (1-t)y \in \mathcal{B}_p[r]\)</span>。这里，<span class="arithmatex">\(tx + (1-t)y\)</span>表示x和y之间的一条线段。</p>
<p>由于x和y满足<span class="arithmatex">\(p\)</span>-范数小于或等于<span class="arithmatex">\(r\)</span>，我们有<span class="arithmatex">\(\|x\|_p \leq r\)</span>和<span class="arithmatex">\(\|y\|_p \leq r\)</span>。</p>
<p>根据<span class="arithmatex">\(t\)</span>，我们可以得到：<span class="arithmatex">\(\|tx + (1-t)y\|_p = t\|x\|_p + (1-t)\|y\|_p \leq tr + (1-t)r = r\)</span></p>
<p>最后一步我们用到的是三角不等式，这里<span class="arithmatex">\(\|\cdot\|_p\)</span>是一个范数，所以满足三角不等式。</p>
<p>因此，我们证明了对于所有的<span class="arithmatex">\(p \geq 1\)</span>，集合<span class="arithmatex">\(\mathcal{B}_p[r]\)</span>是凸的。</p>
<h3 id="1123">练习11.2.3<a class="headerlink" href="#1123" title="Permanent link">⚓︎</a></h3>
<ol>
<li>已知凸函数<span class="arithmatex">\(f\)</span>和<span class="arithmatex">\(g\)</span>表明<span class="arithmatex">\(\mathrm{max}(f, g)\)</span>也是凸函数。证明<span class="arithmatex">\(\mathrm{min}(f, g)\)</span>是非凸的。</li>
</ol>
<p><strong>解答：</strong></p>
<p>在这个问题中中，我们需要分析最大函数（<span class="arithmatex">\(\max\)</span>）和最小函数（<span class="arithmatex">\(\min\)</span>）的凸性质。</p>
<p>首先，我们来分析最大函数的凸性。假设<span class="arithmatex">\(f\)</span>和<span class="arithmatex">\(g\)</span>是凸函数，我们需要证明<span class="arithmatex">\(\max(f, g)\)</span>也是凸函数。</p>
<p>对于任意的<span class="arithmatex">\(x_1, x_2 \in \mathbb{R}\)</span>和<span class="arithmatex">\(0 \leq t \leq 1\)</span>，我们有：</p>
<p><span class="arithmatex">\(\max(t f(x_1) + (1-t) f(x_2), t g(x_1) + (1-t) g(x_2))\)</span></p>
<p>我们可以将其拆分为两个部分：</p>
<ol>
<li><span class="arithmatex">\(\max(t f(x_1), t g(x_1))\)</span></li>
<li><span class="arithmatex">\(\max((1-t) f(x_2), (1-t) g(x_2))\)</span></li>
</ol>
<p>对于第一部分，由于<span class="arithmatex">\(f\)</span>和<span class="arithmatex">\(g\)</span>是凸函数，我们有：</p>
<p><span class="arithmatex">\(t f(x_1) + (1-t) f(x_2) \leq f(t x_1 + (1-t) x_2)\)</span>
<span class="arithmatex">\(t g(x_1) + (1-t) g(x_2) \leq g(t x_1 + (1-t) x_2)\)</span></p>
<p>因此，<span class="arithmatex">\(\max(t f(x_1), t g(x_1)) \leq \max(f(t x_1 + (1-t) x_2), g(t x_1 + (1-t) x_2))\)</span></p>
<p>对于第二部分，我们可以进行类似的推导。</p>
<p>综上所述，我们有：</p>
<p><span class="arithmatex">\(\max(t f(x_1) + (1-t) f(x_2), t g(x_1) + (1-t) g(x_2)) \leq \max(f(t x_1 + (1-t) x_2), g(t x_1 + (1-t) x_2))\)</span></p>
<p>因此，我们证明了最大函数<span class="arithmatex">\(\max(f, g)\)</span>是凸函数。</p>
<p>接下来，我们来证明最小函数<span class="arithmatex">\(\min(f, g)\)</span>是非凸的。</p>
<p>假设<span class="arithmatex">\(f(x) = x\)</span>和<span class="arithmatex">\(g(x) = -x\)</span>，我们可以看到<span class="arithmatex">\(f\)</span>和<span class="arithmatex">\(g\)</span>都是凸函数，因为它们的二阶导数恒为零。</p>
<p>然而，<span class="arithmatex">\(\min(f, g) = \min(x, -x) = -x\)</span>，这是一个非凸函数，因为它的二阶导数为负。</p>
<p>因此，我们证明了最小函数<span class="arithmatex">\(\min(f, g)\)</span>是非凸的。</p>
<h3 id="1124">练习11.2.4<a class="headerlink" href="#1124" title="Permanent link">⚓︎</a></h3>
<p>4.证明Softmax函数的规范化是凸的，即<span class="arithmatex">\(f(x) = \log \sum_i \exp(x_i)\)</span>的凸性。</p>
<p><strong>解答：</strong></p>
<p>思路：证明Softmax函数是凸函数，即证明Softmax函数的二阶导数大于等于0（Hessian矩阵正定）</p>
<p>Softmax函数的定义：<span class="arithmatex">\(f(x) = \log \left(\sum_{i=1}^{n}\exp(x_i)\right)\)</span></p>
<p>求一阶导数：
<span class="arithmatex">\(\frac{\partial f(x)}{\partial x_i} = \frac{1}{\sum_{j=1}^{n}\exp(x_j)} \cdot \left(\text{Softmax}(x_i) - \text{Softmax}^2(x_i)\right)\)</span></p>
<p>求二阶导数：
<span class="arithmatex">\(\frac{\partial^2 f(x)}{\partial x_i^2} = \text{Softmax}(x_i) - \text{Softmax}^2(x_i)\)</span></p>
<p>根据Softmax函数的性质，我们知道<span class="arithmatex">\(\text{Softmax}(x_i) \geq 0\)</span> 且 $ \text{Softmax}(x_i) \leq 1<span class="arithmatex">\(，因此\)</span>\text{Softmax}^2(x_i) \leq \text{Softmax}(x_i)$。</p>
<p>因此，<span class="arithmatex">\(\text{Softmax}(x_i) - \text{Softmax}^2(x_i) \geq 0\)</span>，即Softmax函数的二阶导数大于等于零。</p>
<p>综上所述，我们证明了规范化的Softmax函数<span class="arithmatex">\(f(x) = \log \sum_i \exp(x_i)\)</span>是凸函数，因为它的二阶导数大于等于零。</p>
<h3 id="1125">练习11.2.5<a class="headerlink" href="#1125" title="Permanent link">⚓︎</a></h3>
<p>证明线性子空间<span class="arithmatex">\(X = \{\mathbf{x} | \mathbf{W} \mathbf{x} = \mathbf{b}\}\)</span>是凸集。</p>
<p><strong>解答：</strong></p>
<p>假设<span class="arithmatex">\(X\)</span>非空，即存在至少一个解<span class="arithmatex">\(\mathbf{x}\)</span>满足<span class="arithmatex">\(\mathbf{W} \mathbf{x} = \mathbf{b}\)</span>。这可以通过选择<span class="arithmatex">\(\mathbf{x} = \mathbf{W}^{-1} \mathbf{b}\)</span>来实现，其中<span class="arithmatex">\(\mathbf{W}^{-1}\)</span>表示<span class="arithmatex">\(\mathbf{W}\)</span>的逆矩阵，我们选择任意两个解<span class="arithmatex">\(\mathbf{x}_1\)</span>和<span class="arithmatex">\(\mathbf{x}_2\)</span>，满足<span class="arithmatex">\(\mathbf{W} \mathbf{x}_1 = \mathbf{b}\)</span>和<span class="arithmatex">\(\mathbf{W} \mathbf{x}_2 = \mathbf{b}\)</span>。</p>
<p>我们选择介于<span class="arithmatex">\(\mathbf{x}_1\)</span>和<span class="arithmatex">\(\mathbf{x}_2\)</span>之间的向量<span class="arithmatex">\(\mathbf{y}\)</span>，可以表示为<span class="arithmatex">\(\mathbf{y} = \lambda \mathbf{x}_1 + (1-\lambda) \mathbf{x}_2\)</span>，其中<span class="arithmatex">\(0 \leq \lambda \leq 1\)</span>。</p>
<p>证明<span class="arithmatex">\(\mathbf{W} \mathbf{y} = \mathbf{b}\)</span>过程如下:</p>
<p><span class="arithmatex">\(\mathbf{W} \mathbf{y} = \mathbf{W} (\lambda \mathbf{x}_1 + (1-\lambda) \mathbf{x}_2)\)</span></p>
<p>由于<span class="arithmatex">\(\mathbf{W} \mathbf{x}_1 = \mathbf{b}\)</span>和<span class="arithmatex">\(\mathbf{W} \mathbf{x}_2 = \mathbf{b}\)</span>，可以得到：</p>
<p><span class="arithmatex">\(\mathbf{W} \mathbf{y} = \lambda \mathbf{W} \mathbf{x}_1 + (1-\lambda) \mathbf{W} \mathbf{x}_2 = \lambda \mathbf{b} + (1-\lambda) \mathbf{b} = \mathbf{b}\)</span></p>
<p>因此，对于任意介于<span class="arithmatex">\(\mathbf{x}_1\)</span>和<span class="arithmatex">\(\mathbf{x}_2\)</span>之间的向量<span class="arithmatex">\(\mathbf{y}\)</span>，都有<span class="arithmatex">\(\mathbf{W} \mathbf{y} = \mathbf{b}\)</span>。</p>
<p>根据凸集的定义，我们证明了对于任意两个解<span class="arithmatex">\(\mathbf{x}_1\)</span>和<span class="arithmatex">\(\mathbf{x}_2\)</span>，以及介于<span class="arithmatex">\(\mathbf{x}_1\)</span>和<span class="arithmatex">\(\mathbf{x}_2\)</span>之间的向量<span class="arithmatex">\(\mathbf{y}\)</span>，都有<span class="arithmatex">\(\mathbf{W} \mathbf{y} = \mathbf{b}\)</span>。因此，线性子空间<span class="arithmatex">\(X = \{\mathbf{x} | \mathbf{W} \mathbf{x} = \mathbf{b}\}\)</span>是凸集。</p>
<h3 id="1126">练习11.2.6<a class="headerlink" href="#1126" title="Permanent link">⚓︎</a></h3>
<p>6.证明在线性子空间<span class="arithmatex">\(\mathbf{b} = \mathbf{0}\)</span>的情况下，对于矩阵<span class="arithmatex">\(\mathbf{M}\)</span>的投影<span class="arithmatex">\(\mathrm {Proj} \mathbf{X}\)</span>可以写成<span class="arithmatex">\(\mathbf{M} \mathbf{X}\)</span>。</p>
<p><strong>解答：</strong></p>
<p>1：首先，我们需要了解投影的定义。对于给定的向量<span class="arithmatex">\(\mathbf{v}\)</span>和子空间<span class="arithmatex">\(\mathbf{X}\)</span>，投影<span class="arithmatex">\(\mathrm{Proj} \mathbf{X}\)</span>是指在<span class="arithmatex">\(\mathbf{X}\)</span>上找到与<span class="arithmatex">\(\mathbf{v}\)</span>最接近的向量。</p>
<p>2：在本问题中，我们考虑线性子空间<span class="arithmatex">\(\mathbf{X}\)</span>，其中<span class="arithmatex">\(\mathbf{b} = \mathbf{0}\)</span>。这意味着<span class="arithmatex">\(\mathbf{X}\)</span>包含所有满足<span class="arithmatex">\(\mathbf{M} \mathbf{X} = \mathbf{0}\)</span>的向量<span class="arithmatex">\(\mathbf{X}\)</span>。我们的目标是证明<span class="arithmatex">\(\mathrm{Proj} \mathbf{X}\)</span>可以写成<span class="arithmatex">\(\mathbf{M} \mathbf{X}\)</span>的形式。</p>
<p>3：让我们考虑一个任意的向量<span class="arithmatex">\(\mathbf{v}\)</span>，我们要找到在<span class="arithmatex">\(\mathbf{X}\)</span>上与<span class="arithmatex">\(\mathbf{v}\)</span>最接近的向量。我们将这个最接近的向量表示为<span class="arithmatex">\(\mathbf{p}\)</span>。</p>
<p>4：根据投影的定义，<span class="arithmatex">\(\mathbf{p}\)</span>应满足两个条件：首先，<span class="arithmatex">\(\mathbf{p}\)</span>必须在<span class="arithmatex">\(\mathbf{X}\)</span>中，也就是说，<span class="arithmatex">\(\mathbf{p}\)</span>可以表示为<span class="arithmatex">\(\mathbf{p} = \mathbf{M} \mathbf{X}\)</span>，其中<span class="arithmatex">\(\mathbf{X}\)</span>是<span class="arithmatex">\(\mathbf{X}\)</span>中的向量。其次，<span class="arithmatex">\(\mathbf{p}\)</span>与<span class="arithmatex">\(\mathbf{v}\)</span>之间的差向量<span class="arithmatex">\(\mathbf{v} - \mathbf{p}\)</span>必须与<span class="arithmatex">\(\mathbf{X}\)</span>中的任意向量正交。</p>
<p>5：根据步骤4的条件，我们可以得到以下方程：<span class="arithmatex">\(\mathbf{v} - \mathbf{p} = \mathbf{v} - \mathbf{M} \mathbf{X}\)</span></p>
<p>这个方程表示<span class="arithmatex">\(\mathbf{v} - \mathbf{p}\)</span>与<span class="arithmatex">\(\mathbf{X}\)</span>中的任意向量正交。我们可以将其重写为：<span class="arithmatex">\(\mathbf{M} \mathbf{X} = \mathbf{v} - \mathbf{p}\)</span></p>
<p>6：由于<span class="arithmatex">\(\mathbf{p}\)</span>是与<span class="arithmatex">\(\mathbf{v}\)</span>最接近的<span class="arithmatex">\(\mathbf{X}\)</span>中的向量，我们可以将<span class="arithmatex">\(\mathbf{v} - \mathbf{p}\)</span>看作是<span class="arithmatex">\(\mathbf{X}\)</span>中的最小化残差。</p>
<p>7：现在，我们来考虑<span class="arithmatex">\(\mathbf{X}\)</span>中的向量<span class="arithmatex">\(\mathbf{X}\)</span>，使得<span class="arithmatex">\(\mathbf{M} \mathbf{X} = \mathbf{v} - \mathbf{p}\)</span>。我们可以将其写成<span class="arithmatex">\(\mathbf{M} \mathbf{X} + \mathbf{p} = \mathbf{v}\)</span>。</p>
<p>8：将<span class="arithmatex">\(\mathbf{M} \mathbf{X} + \mathbf{p} = \mathbf{v}\)</span>代入<span class="arithmatex">\(\mathbf{p} = \mathbf{M} \mathbf{X}\)</span>中，我们得到<span class="arithmatex">\(\mathbf{M} \mathbf{X} + \mathbf{M} \mathbf{X} = \mathbf{v}\)</span>，简化为<span class="arithmatex">\(2\mathbf{M} \mathbf{X} = \mathbf{v}\)</span>。</p>
<p>9：最后，我们将<span class="arithmatex">\(2\mathbf{M} \mathbf{X} = \mathbf{v}\)</span>除以2，得到<span class="arithmatex">\(\mathbf{M} \mathbf{X} = \frac{1}{2}\mathbf{v}\)</span>。这表明<span class="arithmatex">\(\mathrm{Proj} \mathbf{X}\)</span>可以写成<span class="arithmatex">\(\mathbf{M} \mathbf{X}\)</span>的形式。</p>
<p>综上所述，在线性子空间<span class="arithmatex">\(\mathbf{X}\)</span>中<span class="arithmatex">\(\mathbf{b} = \mathbf{0}\)</span>的情况下，对于矩阵<span class="arithmatex">\(\mathbf{M}\)</span>的投影<span class="arithmatex">\(\mathrm{Proj} \mathbf{X}\)</span>可以写成<span class="arithmatex">\(\mathbf{M} \mathbf{X}\)</span>的形式，其中<span class="arithmatex">\(\mathbf{X}\)</span>是满足<span class="arithmatex">\(\mathbf{M} \mathbf{X} = \frac{1}{2}\mathbf{v}\)</span>的向量。</p>
<h3 id="1127">练习11.2.7<a class="headerlink" href="#1127" title="Permanent link">⚓︎</a></h3>
<p>7.证明对于凸二次可微函数<span class="arithmatex">\(f\)</span>，对于<span class="arithmatex">\(\xi \in [0, \epsilon]\)</span>，我们可以写成<span class="arithmatex">\(f(x + \epsilon) = f(x) + \epsilon f'(x) + \frac{1}{2} \epsilon^2 f''(x + \xi)\)</span>。</p>
<p><strong>解答：</strong></p>
<p>思路就是利用泰勒展开式和介值定理进行证明：</p>
<p>根据凸二次可微函数的定义，我们知道函数<span class="arithmatex">\(f\)</span>是一个凸函数，并且它的一阶导数<span class="arithmatex">\(f'(x)\)</span>和二阶导数<span class="arithmatex">\(f''(x)\)</span>在定义域内存在。</p>
<p>根据泰勒展开公式，我们可以将函数<span class="arithmatex">\(f(x + \epsilon)\)</span>在点<span class="arithmatex">\(x\)</span>处展开为:</p>
<p><span class="arithmatex">\(f(x + \epsilon) = f(x) + \epsilon f'(x) + \frac{1}{2} \epsilon^2 f''(x) + \frac{1}{6} \epsilon^3 f'''(x + \xi_1)\)</span>，</p>
<p>其中<span class="arithmatex">\(\xi_1 \in (0, \epsilon)\)</span>是介于<span class="arithmatex">\(0\)</span>和<span class="arithmatex">\(\epsilon\)</span>之间的一个变量。</p>
<p>由于函数<span class="arithmatex">\(f\)</span>是凸函数，根据凸函数的性质，我们知道<span class="arithmatex">\(f'''(x)\)</span>是非负的。因此，可以将<span class="arithmatex">\(\frac{1}{6} \epsilon^3 f'''(x + \xi_1)\)</span>写成<span class="arithmatex">\(\frac{1}{2} \epsilon^2 \cdot \frac{1}{3} \cdot \epsilon f'''(x + \xi_1)\)</span>。</p>
<p>根据介值定理，存在一个介于<span class="arithmatex">\(x\)</span>和<span class="arithmatex">\(x + \epsilon\)</span>之间的<span class="arithmatex">\(\xi\)</span>，使得<span class="arithmatex">\(f'''(x + \xi) = f'''(x + \xi_1)\)</span>.</p>
<p>把上面的式子整合一下，将三阶导数的这项与二阶导数这项合并（介值定理），于是我们有</p>
<p><span class="arithmatex">\(f(x + \epsilon) = f(x) + \epsilon f'(x) + \frac{1}{2} \epsilon^2 f''(x) + \frac{1}{2} \epsilon^2 \cdot \frac{1}{3} \cdot \epsilon f'''(x + \xi)\)</span>。</p>
<p>化简上式，我们得到所需的等式:</p>
<p><span class="arithmatex">\(f(x + \epsilon) = f(x) + \epsilon f'(x) + \frac{1}{2} \epsilon^2 f''(x + \xi)\)</span>。</p>
<p>因此，我们证明了对于凸二次可微函数<span class="arithmatex">\(f\)</span>，对于<span class="arithmatex">\(\xi \in [0, \epsilon]\)</span>，等式<span class="arithmatex">\(f(x + \epsilon) = f(x) + \epsilon f'(x) + \frac{1}{2} \epsilon^2 f''(x + \xi)\)</span>成立。</p>
<h3 id="1128">练习11.2.8<a class="headerlink" href="#1128" title="Permanent link">⚓︎</a></h3>
<p>8.给定一个凸集<span class="arithmatex">\(\mathrm{X}\)</span>和两个向量<span class="arithmatex">\(\mathbf{x}\)</span>和<span class="arithmatex">\(\mathbf{y}\)</span>证明了投影不会增加距离，即<span class="arithmatex">\(\|\mathbf{x} - \mathbf{y}\| \geq \|\mathrm{Proj}_\mathrm{X}(\mathbf{x}) - \mathrm{Proj}_\mathrm{X}(\mathbf{y})\|\)</span>。</p>
<p><strong>解答：</strong></p>
<p>给定一个凸集 <span class="arithmatex">\(\mathrm{X}\)</span> 和两个向量 <span class="arithmatex">\(\mathbf{x}\)</span> 和 <span class="arithmatex">\(\mathbf{y}\)</span>，我们可以证明投影不会增加距离的性质。</p>
<p>首先，我们定义 <span class="arithmatex">\(\mathbf{p} = \mathrm{Proj}_\mathrm{X}(\mathbf{x})\)</span> 和 <span class="arithmatex">\(\mathbf{q} = \mathrm{Proj}_\mathrm{X}(\mathbf{y})\)</span>，它们分别是向量 <span class="arithmatex">\(\mathbf{x}\)</span> 和 <span class="arithmatex">\(\mathbf{y}\)</span> 在凸集 <span class="arithmatex">\(\mathrm{X}\)</span> 上的投影。</p>
<p>根据投影的定义，我们知道 <span class="arithmatex">\(\mathbf{p} \in \mathrm{X}\)</span> 和 <span class="arithmatex">\(\mathbf{q} \in \mathrm{X}\)</span>，并且对于任意 <span class="arithmatex">\(\mathbf{v} \in \mathrm{X}\)</span>，有 <span class="arithmatex">\(\|\mathbf{x} - \mathbf{p}\| \leq \|\mathbf{x} - \mathbf{v}\|\)</span> 和 <span class="arithmatex">\(\|\mathbf{y} - \mathbf{q}\| \leq \|\mathbf{y} - \mathbf{v}\|\)</span>。</p>
<p>现在，我们来证明 <span class="arithmatex">\(\|\mathbf{x} - \mathbf{y}\| \geq \|\mathbf{p} - \mathbf{q}\|\)</span>：</p>
<p>根据三角不等式，我们有：
\begin{align<em>}
|\mathbf{x} - \mathbf{y}| &amp; = |\mathbf{x} - \mathbf{p} + \mathbf{p} - \mathbf{q} + \mathbf{q} - \mathbf{y}| \
&amp; \geq |\mathbf{x} - \mathbf{p}| - |\mathbf{p} - \mathbf{q}| + |\mathbf{q} - \mathbf{y}|
\end{align</em>}</p>
<p>由于 <span class="arithmatex">\(\mathbf{p} \in \mathrm{X}\)</span> 和 <span class="arithmatex">\(\mathbf{q} \in \mathrm{X}\)</span>，根据投影的定义，我们知道 <span class="arithmatex">\(\|\mathbf{x} - \mathbf{p}\| \geq \|\mathbf{x} - \mathbf{q}\|\)</span> 和 <span class="arithmatex">\(\|\mathbf{y} - \mathbf{q}\| \geq \|\mathbf{y} - \mathbf{p}\|\)</span>。</p>
<p>将这些不等式代入上面的等式中，我们得到：
\begin{align<em>}
|\mathbf{x} - \mathbf{y}| &amp; \geq |\mathbf{x} - \mathbf{q}| - |\mathbf{p} - \mathbf{q}| + |\mathbf{y} - \mathbf{p}| \
&amp; \geq |\mathbf{p} - \mathbf{q}|
\end{align</em>}</p>
<p>因此，根据以上的推导，我们证明了投影不会增加距离的性质：<span class="arithmatex">\(\|\mathbf{x} - \mathbf{y}\| \geq \|\mathrm{Proj}_\mathrm{X}(\mathbf{x}) - \mathrm{Proj}_\mathrm{X}(\mathbf{y})\|\)</span>。</p>
<h2 id="113">11.3 梯度下降<a class="headerlink" href="#113" title="Permanent link">⚓︎</a></h2>
<h3 id="1131">练习11.3.1<a class="headerlink" href="#1131" title="Permanent link">⚓︎</a></h3>
<p>用不同的学习率和目标函数进行梯度下降实验</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的梯度(导数)</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">gd</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">10.0</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch 10, x: </span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>



<span class="k">def</span> <span class="nf">show_trace</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">results</span><span class="p">)),</span><span class="nb">abs</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">results</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Function&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Trace&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">get_test</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">):</span>
  <span class="n">results</span> <span class="o">=</span> <span class="n">gd</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">)</span>
  <span class="n">show_trace</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># x^2为目标函数</span>
<span class="n">get_test</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_grad</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 10, x: 1.073742
</code></pre></div>
<p><img alt="png" src="../output_46_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">get_test</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_grad</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 10, x: 8.170728
</code></pre></div>
<p><img alt="png" src="../output_47_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">get_test</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_grad</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 10, x: 3.486784
</code></pre></div>
<p><img alt="png" src="../output_48_1.png" /></p>
<p>为了演示非凸函数的梯度下降，考虑函数<span class="arithmatex">\(f(x) = x \cdot \cos(cx)\)</span>，其中<span class="arithmatex">\(c\)</span>为某常数。
这个函数有无穷多个局部最小值。
根据我们选择的学习率，我们最终可能只会得到许多解的一个。
下面的例子说明了（不切实际的）高学习率如何导致较差的局部最小值。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.15</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_grad2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的梯度</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">get_test</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_grad2</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">f2</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 10, x: 8.284779
</code></pre></div>
<p><img alt="png" src="../output_51_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># O目标函数</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的梯度</span>
    <span class="k">return</span> <span class="n">c</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_hess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的Hessian</span>
    <span class="k">return</span> <span class="n">c</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">newton</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">10.0</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">f_hess</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch 10, x:&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">show_trace</span><span class="p">(</span><span class="n">newton</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 10, x: tensor(0.)
</code></pre></div>
<p><img alt="png" src="../output_53_1.png" /></p>
<p>现在让我们考虑一个非凸函数，比如<span class="arithmatex">\(f(x) = x \cos(c x)\)</span>，<span class="arithmatex">\(c\)</span>为某些常数。
请注意在牛顿法中，我们最终将除以Hessian。
这意味着如果二阶导数是负的，<span class="arithmatex">\(f\)</span>的值可能会趋于增加。
这是这个算法的致命缺陷！</p>
<div class="highlight"><pre><span></span><code><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.15</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的梯度</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_hess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的Hessian</span>
    <span class="k">return</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">x</span> <span class="o">*</span> <span class="n">c</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">show_trace</span><span class="p">(</span><span class="n">newton</span><span class="p">(),</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 10, x: tensor(26.8341)
</code></pre></div>
<p><img alt="png" src="../output_56_1.png" /></p>
<h3 id="1132">练习11.3.2<a class="headerlink" href="#1132" title="Permanent link">⚓︎</a></h3>
<p>2.在区间<span class="arithmatex">\([a, b]\)</span>中实现线搜索以最小化凸函数。</p>
<ol>
<li>是否需要导数来进行二分搜索，即决定选择<span class="arithmatex">\([a, (a+b)/2]\)</span>还是<span class="arithmatex">\([(a+b)/2, b]\)</span>。</li>
<li>算法的收敛速度有多快？</li>
<li>实现该算法，并将其应用于求<span class="arithmatex">\(\log (\exp(x) + \exp(-2x -3))\)</span>的最小值。</li>
</ol>
<p><strong>解答：</strong></p>
<p>（1）在进行线搜索时，通常不需要导数来进行二分搜索。二分搜索可以通过选择区间的中点来进行迭代，即选择 [a, (a+b)/2] 或者 [(a+b)/2, b]。通过比较函数在两个点的取值，可以确定下一步的搜索方向。这种方法不需要导数信息，而是通过比较函数值来判断搜索的方向。</p>
<p>（2）收敛速度取决于函数的性质以及选择的搜索策略。对于凸函数，线搜索算法通常具有较好的收敛性能。但是具体的收敛速度还会受到函数的形状、初始搜索区间和采取的搜索策略等因素的影响。</p>
<p>（3）代码实现如下：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">convex_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">line_search</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="mi">4</span>
        <span class="k">if</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">f</span><span class="p">(</span><span class="n">x2</span><span class="p">):</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">x2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">x1</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span>



<span class="k">def</span> <span class="nf">plot_trace</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">result</span><span class="p">):</span>
  <span class="c1"># 可视化搜索过程</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Function&#39;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">[::</span><span class="mi">3</span><span class="p">]):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="p">[</span><span class="n">convex_function</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">convex_function</span><span class="p">(</span><span class="n">b</span><span class="p">)],</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Step </span><span class="si">{</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 在区间 [-5, 5] 上进行线搜索</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">line_search</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">convex_function</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">convex_function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plot_trace</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">results</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="../output_60_0.png" /></p>
<h3 id="1133">练习11.3.3<a class="headerlink" href="#1133" title="Permanent link">⚓︎</a></h3>
<p>设计一个定义在<span class="arithmatex">\(\mathbb{R}^2\)</span>上的目标函数，它的梯度下降非常缓慢。提示：不同坐标的缩放方式不同。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">train_2d</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">f_grad</span><span class="p">:</span>
            <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">, x1: </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">, x2: </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">def</span> <span class="nf">show_trace_2d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">results</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#ff7f0e&#39;</span><span class="p">)</span>
    <span class="n">x1_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">x2_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_range</span><span class="p">,</span> <span class="n">x2_range</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;#1f77b4&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">f_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">f_2d_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gd_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">):</span>
    <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">train_2d</span><span class="p">(</span><span class="n">gd_2d</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_2d_grad</span><span class="p">)</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f_2d</span><span class="p">,</span> <span class="n">results</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 20, x1: -0.057646, x2: -0.000073
</code></pre></div>
<p><img alt="png" src="../output_63_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="c1"># a 和 b为放缩因子</span>
<span class="k">def</span> <span class="nf">slow_f_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span>  <span class="n">a</span><span class="o">*</span><span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span>  <span class="n">b</span><span class="o">*</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">slow_f_2d_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="n">b</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">train_2d</span><span class="p">(</span><span class="n">gd_2d</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">f_grad</span><span class="o">=</span><span class="n">slow_f_2d_grad</span><span class="p">)</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">slow_f_2d</span><span class="p">,</span> <span class="n">results</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 100, x1: -4.092834, x2: -0.000000
</code></pre></div>
<p><img alt="png" src="../output_64_1.png" /></p>
<h3 id="1134">练习11.3.4<a class="headerlink" href="#1134" title="Permanent link">⚓︎</a></h3>
<p>使用预处理实现牛顿方法的轻量版本。
使用对角Hessian作为预条件子。
使用它的绝对值，而不是实际值（可能有符号）。
将此应用于上述问题。
将上述算法应用于多个目标函数（凸或非凸）。如果把坐标旋转 45 度会怎么样？</p>
<p><strong>解答：</strong></p>
<p>预处理：计算和存储完整的Hessian非常昂贵，而改善这个问题的一种方法是“预处理”。
它回避了计算整个Hessian，而只计算“对角线”项，即如下的算法更新：</p>
<div class="arithmatex">\[\mathbf{x} \leftarrow \mathbf{x} - \eta \mathrm{diag}(\mathbf{H})^{-1} \nabla f(\mathbf{x}).\]</div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">newton_preconditioned</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mf">10.0</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">preconditioner</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">f_hess</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># 预条件子为Hessian的绝对值</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">preconditioner</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch 10, x:&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的梯度</span>
    <span class="k">return</span> <span class="n">c</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sinh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_hess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的Hessian</span>
    <span class="k">return</span> <span class="n">c</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosh</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">newton_preconditioned</span><span class="p">()</span>
<span class="n">show_trace</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 10, x: tensor(0.)
</code></pre></div>
<p><img alt="png" src="../output_69_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.15</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的梯度</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_hess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># 目标函数的Hessian</span>
    <span class="k">return</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">x</span> <span class="o">*</span> <span class="n">c</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">newton_preconditioned</span><span class="p">()</span>
<span class="n">show_trace</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 10, x: tensor(20.2219)
</code></pre></div>
<p><img alt="png" src="../output_70_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">train_2d</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">f_grad</span><span class="p">:</span>
            <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">, x1: </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">, x2: </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">def</span> <span class="nf">show_trace_2d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">results</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#ff7f0e&#39;</span><span class="p">)</span>
    <span class="n">x1_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">x2_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1_range</span><span class="p">,</span> <span class="n">x2_range</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;#1f77b4&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">f_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">f_2d_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gd_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">):</span>
    <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">train_2d</span><span class="p">(</span><span class="n">gd_2d</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_2d_grad</span><span class="p">)</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f_2d</span><span class="p">,</span> <span class="n">results</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 20, x1: -0.000000, x2: -0.000073
</code></pre></div>
<p><img alt="png" src="../output_71_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">rotation</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">xa</span> <span class="o">=</span> <span class="p">(</span><span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">xb</span> <span class="o">=</span> <span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">xa</span><span class="p">,</span> <span class="n">xb</span>

<span class="k">def</span> <span class="nf">newton_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">,</span> <span class="n">f_hessian</span><span class="p">):</span>
    <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">h11</span><span class="p">,</span> <span class="n">h12</span><span class="p">,</span> <span class="n">h21</span><span class="p">,</span> <span class="n">h22</span> <span class="o">=</span> <span class="n">f_hessian</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>

    <span class="c1"># 计算预处理的牛顿法步长</span>
    <span class="n">det_hessian</span> <span class="o">=</span> <span class="n">h11</span> <span class="o">*</span> <span class="n">h22</span> <span class="o">-</span> <span class="n">h12</span> <span class="o">*</span> <span class="n">h21</span>
    <span class="n">inv_hessian</span> <span class="o">=</span> <span class="p">(</span><span class="n">h22</span> <span class="o">/</span> <span class="n">det_hessian</span><span class="p">,</span> <span class="o">-</span><span class="n">h12</span> <span class="o">/</span> <span class="n">det_hessian</span><span class="p">,</span> <span class="o">-</span><span class="n">h21</span> <span class="o">/</span> <span class="n">det_hessian</span><span class="p">,</span> <span class="n">h11</span> <span class="o">/</span> <span class="n">det_hessian</span><span class="p">)</span>

    <span class="c1"># 计算更新后的位置</span>
    <span class="n">delta_x1</span> <span class="o">=</span> <span class="n">inv_hessian</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">inv_hessian</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">g2</span>
    <span class="n">delta_x2</span> <span class="o">=</span> <span class="n">inv_hessian</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">inv_hessian</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">g2</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">delta_x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">delta_x2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">f_hessian</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">h11</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">h12</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">h21</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">h22</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="k">return</span> <span class="n">h11</span><span class="p">,</span> <span class="n">h12</span><span class="p">,</span> <span class="n">h21</span><span class="p">,</span> <span class="n">h22</span>

<span class="k">def</span> <span class="nf">train_2d_hessian</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">f_hessian</span><span class="o">=</span><span class="n">f_hessian</span><span class="p">,</span> <span class="n">is_rotation</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">is_rotation</span><span class="p">:</span>
      <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">rotation</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">f_grad</span><span class="p">:</span>
            <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">,</span> <span class="n">f_hessian</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">f_hessian</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">, x1: </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">, x2: </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>


<span class="n">results</span> <span class="o">=</span> <span class="n">train_2d_hessian</span><span class="p">(</span><span class="n">newton_2d</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_2d_grad</span><span class="p">,</span><span class="n">is_rotation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f_2d</span><span class="p">,</span> <span class="n">results</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 20, x1: 0.000000, x2: 0.000000
</code></pre></div>
<p><img alt="png" src="../output_72_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">train_2d_hessian</span><span class="p">(</span><span class="n">newton_2d</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_2d_grad</span><span class="p">,</span><span class="n">is_rotation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f_2d</span><span class="p">,</span> <span class="n">results</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 20, x1: 0.000000, x2: 0.000000
</code></pre></div>
<p><img alt="png" src="../output_73_1.png" /></p>
<h2 id="114">11.4 随机梯度下降<a class="headerlink" href="#114" title="Permanent link">⚓︎</a></h2>
<h3 id="1141">练习11.4.1<a class="headerlink" href="#1141" title="Permanent link">⚓︎</a></h3>
<p>1.尝试不同的随机梯度下降学习率计划和不同的迭代次数进行实验。特别是，根据迭代次数的函数来绘制与最优解(0,0)的距离。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>  <span class="c1"># 目标函数</span>
    <span class="k">return</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">f_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>  <span class="c1"># 目标函数的梯度</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x2</span>

<span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">):</span>
    <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
    <span class="c1"># 模拟有噪声的梯度</span>
    <span class="n">g1</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">g2</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">eta_t</span> <span class="o">=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">lr</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">eta_t</span> <span class="o">*</span> <span class="n">g1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">eta_t</span> <span class="o">*</span> <span class="n">g2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">constant_lr</span><span class="p">():</span>
    <span class="k">return</span> <span class="mi">1</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">constant_lr</span>  <span class="c1"># 常数学习速度</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">train_2d</span><span class="p">(</span><span class="n">sgd</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_grad</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 50, x1: -0.060110, x2: -0.022708
</code></pre></div>
<p><img alt="png" src="../output_77_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">exponential_lr</span><span class="p">():</span>
    <span class="c1"># 在函数外部定义，而在内部更新的全局变量</span>
    <span class="k">global</span> <span class="n">t</span>
    <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span>

<span class="n">t</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">exponential_lr</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">train_2d</span><span class="p">(</span><span class="n">sgd</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_grad</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 1000, x1: -0.720066, x2: -0.101066
</code></pre></div>
<p><img alt="png" src="../output_78_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">polynomial_lr</span><span class="p">():</span>
    <span class="c1"># 在函数外部定义，而在内部更新的全局变量</span>
    <span class="k">global</span> <span class="n">t</span>
    <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">t</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">polynomial_lr</span>
<span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">train_2d</span><span class="p">(</span><span class="n">sgd</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="n">f_grad</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 50, x1: -0.049334, x2: 0.046992
</code></pre></div>
<p><img alt="png" src="../output_79_1.png" /></p>
<h3 id="1142">练习11.4.2<a class="headerlink" href="#1142" title="Permanent link">⚓︎</a></h3>
<p>证明对于函数<span class="arithmatex">\(f(x_1, x_2) = x_1^2 + 2 x_2^2\)</span>而言，向梯度添加正态噪声等同于最小化损失函数<span class="arithmatex">\(f(\mathbf{x}, \mathbf{w}) = (x_1 - w_1)^2 + 2 (x_2 - w_2)^2\)</span>，其中<span class="arithmatex">\(\mathbf{x}\)</span>是从正态分布中提取的</p>
<p><strong>解答：</strong></p>
<p>证明过程如下：</p>
<ol>
<li>首先，计算函数<span class="arithmatex">\(f(x_1, x_2)\)</span>的梯度。根据链式法则，我们有：</li>
</ol>
<p><span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial f}{\partial x_1} = 2x_1\)</span>\)</span>
   <span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial f}{\partial x_2} = 4x_2\)</span>\)</span></p>
<ol>
<li>然后，计算损失函数<span class="arithmatex">\(f(\mathbf{x}, \mathbf{w})\)</span>的梯度。根据链式法则，我们有：</li>
</ol>
<p><span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial f}{\partial w_1} = 2(x_1 - w_1)\)</span>\)</span>
   <span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial f}{\partial w_2} = 4(x_2 - w_2)\)</span>\)</span></p>
<ol>
<li>接下来，我们将向梯度添加正态噪声。设正态噪声为<span class="arithmatex">\(\mathbf{n} = (n_1, n_2)\)</span>，其中<span class="arithmatex">\(n_1\)</span>和<span class="arithmatex">\(n_2\)</span>是从正态分布中提取的随机数。我们可以将梯度添加噪声的操作表示为：</li>
</ol>
<p><span class="arithmatex">\(<span class="arithmatex">\(\mathbf{g} = \left(\frac{\partial f}{\partial x_1} + n_1, \frac{\partial f}{\partial x_2} + n_2\right)\)</span>\)</span></p>
<ol>
<li>最后，我们需要证明向梯度添加正态噪声等同于最小化损失函数。我们将证明两种方法的期望值相等。</li>
</ol>
<p>首先计算向梯度添加噪声的期望值：</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\mathbb{E}[\mathbf{g}] = \left(\mathbb{E}\left[\frac{\partial f}{\partial x_1}\right] + \mathbb{E}[n_1], \mathbb{E}\left[\frac{\partial f}{\partial x_2}\right] + \mathbb{E}[n_2]\right)\)</span>\)</span></p>
<p>由于正态分布的期望值为0，我们有<span class="arithmatex">\(\mathbb{E}[n_1] = \mathbb{E}[n_2] = 0\)</span>。因此，上式可以简化为：</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\mathbb{E}[\mathbf{g}] = \left(\mathbb{E}\left[\frac{\partial f}{\partial x_1}\right], \mathbb{E}\left[\frac{\partial f}{\partial x_2}\right]\right)\)</span>\)</span></p>
<p>接下来计算损失函数的梯度的期望值：</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\mathbb{E}\left[\frac{\partial f}{\partial w_1}\right] = \mathbb{E}\left[2(x_1 - w_1)\right] = 2\mathbb{E}[x_1] - 2w_1\)</span>\)</span>
   <span class="arithmatex">\(<span class="arithmatex">\(\mathbb{E}\left[\frac{\partial f}{\partial w_2}\right] = \mathbb{E}\left[4(x_2 - w_2)\right] = 4\mathbb{E}[x_2] - 4w_2\)</span>\)</span></p>
<p>最后，我们需要证明<span class="arithmatex">\(\mathbb{E}[\mathbf{g}] = \left(\mathbb{E}\left[\frac{\partial f}{\partial x_1}\right], \mathbb{E}\left[\frac{\partial f}{\partial x_2}\right]\right) = \left(2\mathbb{E}[x_1] - 2w_1, 4\mathbb{E}[x_2] - 4w_2\right)\)</span>。</p>
<p>由于<span class="arithmatex">\(x_1\)</span>和<span class="arithmatex">\(x_2\)</span>是从正态分布中提取的随机数，其期望值等于其均值。因此，我们有<span class="arithmatex">\(\mathbb{E}[x_1] = \mathbb{E}[x_2] = 0\)</span>。代入上式，我们得到：</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(\mathbb{E}[\mathbf{g}] = \left(2\mathbb{E}[x_1] - 2w_1, 4\mathbb{E}[x_2] - 4w_2\right) = (0 - 2w_1, 0 - 4w_2) = (-2w_1, -4w_2)\)</span>\)</span></p>
<p>这与损失函数的梯度<span class="arithmatex">\(\left(\frac{\partial f}{\partial w_1}, \frac{\partial f}{\partial w_2}\right) = (2(x_1 - w_1), 4(x_2 - w_2))\)</span>相等。</p>
<p>因此，我们证明了向梯度添加正态噪声等同于最小化损失函数。</p>
<h3 id="1143">练习11.4.3<a class="headerlink" href="#1143" title="Permanent link">⚓︎</a></h3>
<ol>
<li>从<span class="arithmatex">\(\{(x_1, y_1), \ldots, (x_n, y_n)\}\)</span>分别使用替换方法以及不替换方法进行采样时，比较随机梯度下降的收敛性。</li>
</ol>
<p><strong>解答：</strong></p>
<p>在随机梯度下降算法中，替换方法和不替换方法是两种不同的采样方式，对于每一次迭代，都会从样本集中随机选择一个样本进行梯度计算和参数更新。</p>
<ul>
<li>
<p>在替换方法中，每次选择的样本都是独立随机选择的，即每次选择的样本都可能是之前已经选择过的样本。这种方法的优点是可以更快地收敛，因为每次迭代都会使用更多的样本信息进行参数更新。然而，由于每次选择的样本都是独立的，可能会导致某些样本被选择多次，而其他样本则很少被选择到，从而导致样本的分布不均匀，可能会影响算法的收敛性和泛化性能。</p>
</li>
<li>
<p>在不替换方法中，每次选择的样本都是不重复的，即每次选择的样本都不会是之前已经选择过的样本。这种方法的优点是可以保证每个样本都被使用到，从而可以更好地保证样本的分布均匀性。然而，由于每次迭代都只使用一个样本进行参数更新，可能会导致参数更新的方向不够准确，从而影响算法的收敛速度。</p>
</li>
</ul>
<p>因此，替换方法和不替换方法在随机梯度下降算法的收敛性上存在一定的差异。替换方法收敛速度较快，但可能会导致样本分布不均匀；不替换方法可以保证样本分布均匀，但收敛速度较慢。在实际应用中，可以根据具体的问题和需求选择合适的采样方法。</p>
<h3 id="1144">练习11.4.4<a class="headerlink" href="#1144" title="Permanent link">⚓︎</a></h3>
<p>4.如果某些梯度（或者更确切地说与之相关的某些坐标）始终比所有其他梯度都大，将如何更改随机梯度下降求解器？</p>
<p><strong>解答：</strong></p>
<p>如果某些梯度（或者与之相关的某些坐标）始终比所有其他梯度都大，这可能意味着这些参数的更新过程非常不稳定，可能导致训练过程无法收敛或者收敛非常缓慢。在这种情况下，以下步骤来改进随机梯度下降（SGD）求解器：</p>
<ol>
<li>
<p>梯度截断：首先，可以尝试使用梯度截断技术来限制梯度的大小。通过设置一个阈值，当梯度超过该阈值时，将其缩小到阈值范围内。这样可以避免梯度爆炸的问题。</p>
</li>
<li>
<p>学习率调整：如果某些梯度始终比其他梯度大，可能意味着学习率过大。尝试减小学习率，可以通过手动设置较小的学习率或者使用自适应学习率算法（如Adam、Adagrad等）来自动调整学习率。</p>
</li>
<li>
<p>批量规范化：考虑使用批量规范化（Batch Normalization）技术来调整梯度。批量规范化可以通过对每个批次的输入进行归一化来减少梯度的变化范围，从而提高训练的稳定性。</p>
</li>
<li>
<p>参数初始化：检查参数的初始化方式。如果某些参数初始值较大，可能导致初始梯度也很大，进而影响训练过程。尝试使用较小的初始值或者使用一些特定的参数初始化方法（如Xavier初始化）来减小梯度的大小。</p>
</li>
<li>
<p>梯度下降变体：考虑使用其他的梯度下降变体。例如，可以尝试使用带动量的随机梯度下降（SGD with Momentum）或者自适应学习率的优化算法（如Adam、Adagrad等），这些算法可以更好地处理不稳定的梯度。</p>
</li>
<li>
<p>调整网络架构：如果上述方法仍然无法解决问题，可以考虑调整网络架构。可能需要重新设计网络结构，调整层数、神经元数目或者激活函数等，以便更好地处理梯度的不稳定性。</p>
</li>
</ol>
<h3 id="1145">练习11.4.5<a class="headerlink" href="#1145" title="Permanent link">⚓︎</a></h3>
<p>5.假设<span class="arithmatex">\(f(x) = x^2 (1 + \sin x)\)</span>。<span class="arithmatex">\(f\)</span>有多少局部最小值？请试着改变<span class="arithmatex">\(f\)</span>以尽量减少它需要评估所有局部最小值的方式。</p>
<p><strong>解答：</strong></p>
<p>要确定函数<span class="arithmatex">\(f(x) = x^2 (1 + \sin x)\)</span>的局部最小值的数量，我们需要找到函数的导数，并找到导数为零的点。在这些点上，函数可能具有局部最小值。</p>
<p>首先，我们计算函数<span class="arithmatex">\(f(x)\)</span>的导数：
$$
f'(x) = 2x(1 + \sin x) + x^2 \cos x
$$</p>
<p>要找到导数为零的点，我们解方程<span class="arithmatex">\(f'(x) = 0\)</span>：
$$
2x(1 + \sin x) + x^2 \cos x = 0
$$</p>
<p>这是一个非线性方程，很难通过解析方法找到其解。因此，我们可以通过数值方法来近似解。</p>
<p>为了尽量减少需要评估所有局部最小值的方式，我们可以使用以下策略：
1. 观察函数图像：首先，我们可以绘制函数<span class="arithmatex">\(f(x)\)</span>的图像，观察函数的形状和曲线。这可以帮助我们获得关于函数局部最小值的直观理解，并指导我们在哪些区域进行进一步的分析。
2. 初始点选择：根据观察到的函数图像，我们可以选择一些可能的初始点，这些点可能是导数为零的点。这样可以帮助我们避免评估所有可能的局部最小值。
3. 迭代优化算法：使用迭代优化算法（如梯度下降、牛顿法、拟牛顿法等）来搜索函数的最小值。通过选择适当的初始点和合适的优化算法，我们可以在较少的迭代次数内找到局部最小值。</p>
<p>请注意，由于函数<span class="arithmatex">\(f(x)\)</span>的非线性性质，可能存在多个局部最小值。因此，找到所有局部最小值可能需要一定的计算成本。通过上述策略，我们可以尽量减少评估所有局部最小值的方式，并更有效地搜索函数的最小值。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">f_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">eta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">f_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10.0</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">g</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">lr</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;epoch 10, x: </span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s1">f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="k">def</span> <span class="nf">constant_lr</span><span class="p">():</span>
    <span class="k">return</span> <span class="mf">0.8</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">constant_lr</span>  <span class="c1"># 常数学习速度</span>
<span class="c1"># lr = exponential_lr</span>
<span class="n">show_trace</span><span class="p">(</span><span class="n">sgd</span><span class="p">(</span><span class="n">eta</span><span class="p">,</span> <span class="n">f_grad</span><span class="p">),</span><span class="n">f</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;ipython-input-129-4e17c7a29cae&gt;:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x = torch.tensor(x, requires_grad=True)  # 将x转换为torch张量，并设置requires_grad=True以计算导数


epoch 10, x: 10.993654
</code></pre></div>
<p><img alt="png" src="../output_92_2.png" /></p>
<h2 id="115">11.5 小批量随机梯度下降<a class="headerlink" href="#115" title="Permanent link">⚓︎</a></h2>
<h3 id="1151">练习11.5.1<a class="headerlink" href="#1151" title="Permanent link">⚓︎</a></h3>
<p>1.修改批量大小和学习率，并观察目标函数值的下降率以及每个迭代轮数消耗的时间</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">d2l</span><span class="o">.</span><span class="n">DATA_HUB</span><span class="p">[</span><span class="s1">&#39;airfoil&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">DATA_URL</span> <span class="o">+</span> <span class="s1">&#39;airfoil_self_noise.dat&#39;</span><span class="p">,</span><span class="s1">&#39;76e5be1548fd8222e5074cf0faae75edff8cf93f&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1500</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;airfoil&#39;</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="c1"># print(data.shape)</span>
    <span class="n">data_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_array</span><span class="p">((</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                               <span class="n">batch_size</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train_ch11</span><span class="p">(</span><span class="n">trainer_fn</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">,</span>
               <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="c1"># 初始化模型</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                     <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">net</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">X</span><span class="p">:</span> <span class="n">d2l</span><span class="o">.</span><span class="n">linreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">d2l</span><span class="o">.</span><span class="n">squared_loss</span>
    <span class="c1"># 训练模型</span>
    <span class="n">animator</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span>
                            <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mf">0.22</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">])</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">timer</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Timer</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">trainer_fn</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">)</span>
            <span class="n">n</span> <span class="o">+=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
                <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">data_iter</span><span class="p">),</span>
                             <span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">evaluate_loss</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">),))</span>
                <span class="n">timer</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;loss: </span><span class="si">{</span><span class="n">animator</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">timer</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> sec/epoch&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">timer</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(),</span> <span class="n">animator</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train_sgd</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_ch11</span><span class="p">(</span>
        <span class="n">sgd</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">gd_res</span> <span class="o">=</span> <span class="n">train_sgd</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.258, 0.109 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_101_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">sgd_res</span> <span class="o">=</span> <span class="n">train_sgd</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.243, 0.121 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_102_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mini1_res</span> <span class="o">=</span> <span class="n">train_sgd</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span> <span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.249, 0.022 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_103_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mini2_res</span> <span class="o">=</span> <span class="n">train_sgd</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span> <span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.242, 0.012 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_104_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="c1"># 对比4种优化方式</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gd_res</span><span class="p">,</span> <span class="n">sgd_res</span><span class="p">,</span> <span class="n">mini1_res</span><span class="p">,</span> <span class="n">mini2_res</span><span class="p">))),</span>
         <span class="s1">&#39;time (sec)&#39;</span><span class="p">,</span> <span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
         <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;gd&#39;</span><span class="p">,</span> <span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="s1">&#39;batch size=100&#39;</span><span class="p">,</span> <span class="s1">&#39;batch size=10&#39;</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="svg" src="../output_105_0.svg" /></p>
<h3 id="1152">练习11.5.2<a class="headerlink" href="#1152" title="Permanent link">⚓︎</a></h3>
<p>2.将小批量随机梯度下降与实际从训练集中取样替换的变体进行比较。会看出什么？</p>
<p><strong>解答：</strong></p>
<p>小批量随机梯度下降（mini-batch stochastic gradient descent）是梯度下降的一种变体，它在每次迭代中随机选择一小批训练样本来计算梯度并更新模型参数。</p>
<p>实际从训练集中取样替换的变体（actual sampling with replacement variant）是指每次迭代中从训练集中随机选择一个样本计算梯度并更新模型参数。</p>
<p>比较两者可以得出以下观察：</p>
<ol>
<li>
<p>计算效率：小批量随机梯度下降通常比实际取样替换的变体更高效。因为小批量随机梯度下降每次迭代计算一小批样本的梯度，可以充分利用矩阵运算的并行性，加快计算速度。而实际取样替换的变体需要每次迭代计算一个样本的梯度，计算效率较低。</p>
</li>
<li>
<p>收敛速度：小批量随机梯度下降通常比实际取样替换的变体更快收敛。因为小批量随机梯度下降每次迭代使用多个样本的梯度更新模型参数，可以更准确地指导模型向最优解收敛。而实际取样替换的变体每次迭代只使用一个样本的梯度，可能会受到单个样本噪声的干扰，导致收敛速度较慢。</p>
</li>
<li>
<p>模型稳定性：小批量随机梯度下降通常比实际取样替换的变体更稳定。因为小批量随机梯度下降使用多个样本的梯度进行更新，可以减少单个样本的噪声对模型参数的影响，提高模型的稳定性。而实际取样替换的变体每次迭代只使用一个样本的梯度，容易受到噪声的干扰，导致模型参数波动较大。</p>
</li>
</ol>
<p>综上所述，小批量随机梯度下降相对于实际取样替换的变体在计算效率、收敛速度和模型稳定性上都有优势。因此，在实际应用中，小批量随机梯度下降更常用和推荐。</p>
<h3 id="1153">练习11.5.3<a class="headerlink" href="#1153" title="Permanent link">⚓︎</a></h3>
<p>一个邪恶的精灵在没通知你的情况下复制了你的数据集（即每个观测发生两次，数据集增加到原始大小的两倍，但没有人告诉你）。随机梯度下降、小批量随机梯度下降和梯度下降的表现将如何变化？</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_data_ch11_duplicate</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1500</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;airfoil&#39;</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="c1"># data = torch.cat((data, data), dim=0) # duplicate the data</span>
    <span class="n">data_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_array</span><span class="p">((</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span>

<span class="k">def</span> <span class="nf">train_sgd_duplicate</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">get_data_ch11_duplicate</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_ch11</span><span class="p">(</span>
        <span class="n">sgd</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mini_sgd</span> <span class="o">=</span> <span class="n">train_sgd_duplicate</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span> <span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.242, 0.012 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_112_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">sgd1</span> <span class="o">=</span> <span class="n">train_sgd_duplicate</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span> <span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.317, 0.150 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_113_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">sgd</span> <span class="o">=</span> <span class="n">train_sgd_duplicate</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span> <span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.244, 0.113 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_114_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">gd</span> <span class="o">=</span> <span class="n">train_sgd_duplicate</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.247, 0.047 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_115_1.svg" /></p>
<h2 id="116">11.6 动量法<a class="headerlink" href="#116" title="Permanent link">⚓︎</a></h2>
<h3 id="1161">练习11.6.1<a class="headerlink" href="#1161" title="Permanent link">⚓︎</a></h3>
<p>使用动量超参数和学习率的其他组合，观察和分析不同的实验结果。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">init_momentum_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">):</span>
    <span class="n">v_w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">v_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">v_w</span><span class="p">,</span> <span class="n">v_b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sgd_momentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">v</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;momentum&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">p</span><span class="p">[:]</span> <span class="o">-=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">train_momentum</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">sgd_momentum</span><span class="p">,</span> <span class="n">init_momentum_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
                   <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="n">momentum</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span>
                   <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>

<span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">train_momentum</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.243, 0.196 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_120_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">train_momentum</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.247, 0.165 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_121_1.svg" /></p>
<h3 id="1162">练习11.6.2<a class="headerlink" href="#1162" title="Permanent link">⚓︎</a></h3>
<p>试试梯度下降和动量法来解决一个二次问题，其中有多个特征值，即<span class="arithmatex">\(f(x) = \frac{1}{2} \sum_i \lambda_i x_i^2\)</span>，例如<span class="arithmatex">\(\lambda_i = 2^{-i}\)</span>。绘制出<span class="arithmatex">\(x\)</span>的值在初始化<span class="arithmatex">\(x_i = 1\)</span>时如何下降。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">quadratic_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">lambdas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">feature_dim</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lambdas</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">lambdas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">feature_dim</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">lambdas</span> <span class="o">*</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">params</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">init_momentum_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">v</span>

<span class="k">def</span> <span class="nf">sgd_momentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;momentum&#39;</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">states</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">states</span> <span class="o">+</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">states</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">mom</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">feature_dim</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 设置特征维度为10，可以根据实际情况进行调整</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mom</span><span class="p">:</span>
        <span class="n">states</span> <span class="o">=</span> <span class="n">init_momentum_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

    <span class="n">x_values</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">quadratic_function</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="n">momentum</span><span class="p">})</span>
        <span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

        <span class="n">x_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">x_values</span>

<span class="c1"># 使用SGD进行训练</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">x_values_sgd</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">sgd</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>

<span class="c1"># 使用带动量的SGD进行训练</span>
<span class="n">x_values_momentum</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">sgd_momentum</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="c1"># 绘制x值的下降情况</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_values_sgd</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_values_momentum</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SGD with Momentum&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;x value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Descent of x values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="../output_125_0.png" /></p>
<h3 id="1163">练习11.6.3<a class="headerlink" href="#1163" title="Permanent link">⚓︎</a></h3>
<p>推导<span class="arithmatex">\(h(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{Q} \mathbf{x} + \mathbf{x}^\top \mathbf{c} + b\)</span>的最小值和最小化器。</p>
<p><strong>解答：</strong></p>
<div class="arithmatex">\[h(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{Q} \mathbf{x} + \mathbf{x}^\top \mathbf{c} + b.\]</div>
<p>这是一个普通的二次函数。
对于正定矩阵<span class="arithmatex">\(\mathbf{Q} \succ 0\)</span>，即对于具有正特征值的矩阵，有</p>
<p>最小化器为<span class="arithmatex">\(\mathbf{x}^* = -\mathbf{Q}^{-1} \mathbf{c}\)</span>，最小值为<span class="arithmatex">\(b - \frac{1}{2} \mathbf{c}^\top \mathbf{Q}^{-1} \mathbf{c}\)</span>。</p>
<p>证明如下：
给定函数：</p>
<div class="arithmatex">\[h(\mathbf{x}) = \frac{1}{2} \mathbf{x}^\top \mathbf{Q} \mathbf{x} + \mathbf{x}^\top \mathbf{c} + b\]</div>
<p>其中，<span class="arithmatex">\(\mathbf{x} \in \mathbb{R}^n\)</span>, <span class="arithmatex">\(\mathbf{Q}\)</span> 是一个 <span class="arithmatex">\(n \times n\)</span> 的正定矩阵，<span class="arithmatex">\(\mathbf{c} \in \mathbb{R}^n\)</span>, <span class="arithmatex">\(b\)</span> 是一个实数。</p>
<p>求解步骤：</p>
<p><strong>Step 1: 求梯度</strong></p>
<p>首先计算 <span class="arithmatex">\(h(\mathbf{x})\)</span> 对 <span class="arithmatex">\(\mathbf{x}\)</span> 的梯度：</p>
<div class="arithmatex">\[
\nabla h(\mathbf{x}) = \nabla \left( \frac{1}{2} \mathbf{x}^\top \mathbf{Q} \mathbf{x} + \mathbf{x}^\top \mathbf{c} + b \right)
= \frac{1}{2} (\mathbf{Q} + \mathbf{Q}^\top) \mathbf{x} + \mathbf{c}.
\]</div>
<p>由于 <span class="arithmatex">\(\mathbf{Q}\)</span> 是对称矩阵，所以 <span class="arithmatex">\(\mathbf{Q} + \mathbf{Q}^\top = 2\mathbf{Q}\)</span>。</p>
<p>因此，梯度可以简化为：</p>
<div class="arithmatex">\[
\nabla h(\mathbf{x}) = \mathbf{Qx} + \mathbf{c}.
\]</div>
<p><strong>Step 2: 解析解</strong></p>
<p>要找到使梯度为零的 <span class="arithmatex">\(\mathbf{x}\)</span>，我们解以下方程：</p>
<div class="arithmatex">\[
\mathbf{Qx} + \mathbf{c} = 0.
\]</div>
<p>解得最小化器为：</p>
<div class="arithmatex">\[
\mathbf{x}^* = -\mathbf{Q}^{-1} \mathbf{c}.
\]</div>
<p>这给出了使 <span class="arithmatex">\(h(\mathbf{x})\)</span> 取得最小值的临界点。</p>
<p><strong>Step 3: 计算最小值</strong></p>
<p>将临界点 <span class="arithmatex">\(\mathbf{x}^* = -\mathbf{Q}^{-1} \mathbf{c}\)</span> 代入 <span class="arithmatex">\(h(\mathbf{x})\)</span>：</p>
<div class="arithmatex">\[
h(\mathbf{x}^*) = \frac{1}{2} \left( -\mathbf{Q}^{-1} \mathbf{c} \right)^\top \mathbf{Q} \left( -\mathbf{Q}^{-1} \mathbf{c} \right) + \left( -\mathbf{Q}^{-1} \mathbf{c} \right)^\top \mathbf{c} + b.
\]</div>
<p>经过推导和化简，我们得到最小值为：</p>
<div class="arithmatex">\[
h(\mathbf{x}^*) = b - \frac{1}{2} \mathbf{c}^\top \mathbf{Q}^{-1} \mathbf{c}.
\]</div>
<p>以上是对给定函数中最小化器和最小值的证明。</p>
<h3 id="1164">练习11.6.4<a class="headerlink" href="#1164" title="Permanent link">⚓︎</a></h3>
<p>当我们执行带动量法的随机梯度下降时会有什么变化？当我们使用带动量法的小批量随机梯度下降时会发生什么？试验参数如何？</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train_sgd</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_ch11</span><span class="p">(</span>
        <span class="n">sgd</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_momentum</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">sgd_momentum</span><span class="p">,</span> <span class="n">init_momentum_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
                   <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="n">momentum</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span>
                   <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_momentum_mini_sgd</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_ch11</span><span class="p">(</span><span class="n">sgd_momentum</span><span class="p">,</span> <span class="n">init_momentum_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
                   <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="n">momentum</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span>
                   <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">train_momentum_mini_sgd</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span> <span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.256, 0.010 sec/epoch





([0.009358644485473633,
  0.018602371215820312,
  0.033394813537597656,
  0.046602725982666016,
  0.05589103698730469,
  0.06343984603881836,
  0.07286882400512695,
  0.0819542407989502,
  0.0899808406829834,
  0.1012260913848877,
  0.10823273658752441,
  0.11756467819213867,
  0.12543272972106934,
  0.13646173477172852,
  0.14547348022460938,
  0.16164088249206543,
  0.16751933097839355,
  0.17676424980163574,
  0.1831367015838623,
  0.19231414794921875,
  0.19889307022094727,
  0.20941829681396484,
  0.22132658958435059,
  0.23333287239074707,
  0.24092578887939453,
  0.255115270614624,
  0.2617835998535156,
  0.26881861686706543,
  0.28046107292175293,
  0.2882564067840576,
  0.29537439346313477,
  0.312119722366333,
  0.32276248931884766,
  0.33200669288635254,
  0.3423175811767578,
  0.3501245975494385,
  0.35841798782348633,
  0.37256574630737305,
  0.3837928771972656,
  0.39443349838256836,
  0.4036686420440674,
  0.4134087562561035,
  0.42447924613952637,
  0.4357783794403076,
  0.44672155380249023,
  0.46033191680908203,
  0.4698967933654785,
  0.4780442714691162,
  0.4852755069732666,
  0.4953904151916504,
  0.5065402984619141,
  0.5198330879211426,
  0.5311398506164551,
  0.5434770584106445,
  0.5589005947113037,
  0.5658104419708252,
  0.5733516216278076,
  0.5833685398101807,
  0.5928308963775635,
  0.6005630493164062,
  0.6070787906646729,
  0.6146812438964844,
  0.6297750473022461,
  0.6401479244232178,
  0.647824764251709,
  0.6550576686859131,
  0.662858247756958,
  0.6741726398468018,
  0.6813902854919434,
  0.6884579658508301,
  0.6955902576446533,
  0.7028961181640625,
  0.7094995975494385,
  0.7167482376098633,
  0.7242491245269775],
 [0.3974764544169108,
  0.2725127175649007,
  0.2528282658259074,
  0.2785123583475749,
  0.27809609826405846,
  0.2535492318471273,
  0.24598347314198812,
  0.24931845156351726,
  0.26698224767049156,
  0.25247500546773277,
  0.2546057071685791,
  0.24426489384969075,
  0.24444231796264648,
  0.24503566646575928,
  0.2445207373301188,
  0.25663725852966307,
  0.25084531688690187,
  0.24612022908528647,
  0.26928609625498456,
  0.25394261614481606,
  0.2530341294606527,
  0.26565273984273274,
  0.2493351879119873,
  0.2496158889134725,
  0.2568613115946452,
  0.25462852414449055,
  0.24943667538960776,
  0.25554658444722494,
  0.257337074915568,
  0.25179729493459063,
  0.24987802982330323,
  0.2575841337839762,
  0.24449276542663576,
  0.2586565481821696,
  0.25465552202860514,
  0.24685403060913086,
  0.2508957430521647,
  0.2500980275472005,
  0.2459493989944458,
  0.24759188842773439,
  0.2516881062189738,
  0.24578703180948894,
  0.2524076124827067,
  0.2485795955657959,
  0.2514031136830648,
  0.24766394233703612,
  0.26300604693094887,
  0.24668835322062174,
  0.2517896172205607,
  0.25997326310475666,
  0.2616598523457845,
  0.2500342502593994,
  0.24844182936350503,
  0.24852244249979655,
  0.2565685421625773,
  0.24900156593322753,
  0.25331158002217613,
  0.2582071320215861,
  0.2454735590616862,
  0.2615393489201864,
  0.2531545317967733,
  0.24621759796142578,
  0.24984279123942058,
  0.25137666257222496,
  0.24336035919189453,
  0.246310879389445,
  0.24855825265248616,
  0.24608558495839436,
  0.25479423332214357,
  0.24780901940663655,
  0.246378111521403,
  0.25880028247833253,
  0.24822867806752522,
  0.24953290049235027,
  0.25573611036936444])
</code></pre></div>
<p><img alt="svg" src="../output_132_2.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">train_momentum_mini_sgd</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span> <span class="p">,</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.247, 0.121 sec/epoch





([0.1437983512878418,
  0.23876953125,
  0.3465588092803955,
  0.4689216613769531,
  0.5945520401000977,
  0.6977694034576416,
  0.7939465045928955,
  0.8971412181854248,
  1.0060510635375977,
  1.1108214855194092,
  1.2186241149902344,
  1.3186967372894287,
  1.4275116920471191,
  1.541830062866211,
  1.6460797786712646,
  1.7679290771484375,
  1.9159116744995117,
  2.107617139816284,
  2.2708587646484375,
  2.3700742721557617,
  2.4778153896331787,
  2.600879430770874,
  2.716142177581787,
  2.8188159465789795,
  2.9307990074157715,
  3.0386407375335693,
  3.139930009841919,
  3.244142770767212,
  3.349316358566284,
  3.4544832706451416,
  3.55359148979187,
  3.6574454307556152,
  3.756129026412964,
  3.8823087215423584,
  4.006078243255615,
  4.142263412475586,
  4.329652786254883,
  4.504828453063965,
  4.681985855102539,
  4.8113853931427,
  4.931288957595825,
  5.035146474838257,
  5.144930839538574,
  5.270723342895508,
  5.3815460205078125,
  5.501541614532471,
  5.614099502563477,
  5.725754261016846,
  5.834448337554932,
  5.9411046504974365,
  6.046403646469116,
  6.164094924926758,
  6.309192657470703,
  6.44723916053772,
  6.617693901062012,
  6.774671316146851,
  6.926396608352661,
  7.028913497924805,
  7.135705232620239,
  7.242016315460205,
  7.357810974121094,
  7.4768359661102295,
  7.58343505859375,
  7.708967447280884,
  7.8075830936431885,
  7.9178478717803955,
  8.038088083267212,
  8.141481399536133,
  8.241400957107544,
  8.344741344451904,
  8.44347596168518,
  8.57492184638977,
  8.730757236480713,
  8.879141092300415,
  9.043273210525513],
 [0.26351944593764853,
  0.25744831538712865,
  0.2547714674251154,
  0.24555723982692299,
  0.24830448408830158,
  0.25225604008439284,
  0.2507579025293263,
  0.26344057983656055,
  0.2432194465350743,
  0.24305821061955082,
  0.24753501967596003,
  0.24369537239232644,
  0.24663906080932557,
  0.2458394198807981,
  0.24651032202922613,
  0.24914990866541326,
  0.256028353498533,
  0.24326733626826982,
  0.2515419778360968,
  0.24751185537618964,
  0.25789616690435807,
  0.2598427818085834,
  0.24979477707876027,
  0.24833421818372753,
  0.2525945635835756,
  0.24606020263176262,
  0.24757738642838792,
  0.24802533763249834,
  0.24417312240890401,
  0.24354536420871198,
  0.24306986984423284,
  0.25256286450428805,
  0.2452697582489815,
  0.2498070247037054,
  0.2477416425259465,
  0.24696074724659692,
  0.24767021993421426,
  0.24867795867149733,
  0.2459862888233667,
  0.2504661845744592,
  0.2469240744031162,
  0.259249572535984,
  0.24475855154850523,
  0.24498729424860213,
  0.25325031320416175,
  0.2555435604985836,
  0.24239871102465743,
  0.25511556658843454,
  0.2562415500064476,
  0.25311547449938104,
  0.24503556964262574,
  0.24343617042611024,
  0.2449727825322109,
  0.2434668858061311,
  0.2502309588126848,
  0.2475542082262777,
  0.25012593984859516,
  0.2464409174058081,
  0.24533545937558096,
  0.24491058229468793,
  0.251677157485663,
  0.24796266144135878,
  0.25056295511015014,
  0.2456515950089087,
  0.24879644469594578,
  0.2483955076197434,
  0.25112281626624555,
  0.25395571238755293,
  0.24491300892982668,
  0.24256010347193377,
  0.24472533344275849,
  0.2478103319378337,
  0.24274485204660504,
  0.24522362426683034,
  0.24676793082665607])
</code></pre></div>
<p><img alt="svg" src="../output_133_2.svg" /></p>
<h2 id="117-adagrad">11.7 AdaGrad算法<a class="headerlink" href="#117-adagrad" title="Permanent link">⚓︎</a></h2>
<h3 id="1171">练习11.7.1<a class="headerlink" href="#1171" title="Permanent link">⚓︎</a></h3>
<p>证明对于正交矩阵<span class="arithmatex">\(\mathbf{U}\)</span>和向量<span class="arithmatex">\(\mathbf{c}\)</span>，以下等式成立：<span class="arithmatex">\(\|\mathbf{c} - \mathbf{\delta}\|_2 = \|\mathbf{U} \mathbf{c} - \mathbf{U} \mathbf{\delta}\|_2\)</span>。为什么这意味着在变量的正交变化之后，扰动的程度不会改变？</p>
<p><strong>解答：</strong></p>
<p>首先证明该等式成立：</p>
<p>等式左侧可以展开为：</p>
<div class="arithmatex">\[||c - \delta\|^2 = (c - \delta)^T(c - \delta) = c^Tc - 2c^T\delta + \delta^T\delta\]</div>
<p>等式右侧可以展开为：</p>
<div class="arithmatex">\[\|Uc - U\delta\|^2 = (Uc - U\delta)^T(Uc - U\delta) = c^TU^TUc - 2c^TU^TU\delta + \delta^TU^TU\delta\]</div>
<p>由于 <span class="arithmatex">\(\mathbf{U}\)</span> 为正交矩阵，即：</p>
<div class="arithmatex">\[U^TU = I\]</div>
<p>所以可得：</p>
<div class="arithmatex">\[c^TU^TUc = c^Tc, c^TU^TU\delta = c^T\delta, \delta^TU^TU\delta = \delta^T\delta\]</div>
<p>带入以上展开式可证：</p>
<div class="arithmatex">\[\|\mathbf{c} - \mathbf{\delta}\|_2 = \|\mathbf{U} \mathbf{c} - \mathbf{U} \mathbf{\delta}\|_2\]</div>
<p>为什么这意味着在变量的正交变化之后，扰动的程度不会改变：</p>
<p>我们可以将扰动的程度理解为变换之后的向量与原向量的距离，即假设我们有扰动变量 <span class="arithmatex">\(\delta\)</span>，那么扰动的程度即为原变量 <span class="arithmatex">\(x\)</span> 与扰动后变量 <span class="arithmatex">\(x+\delta\)</span> 的距离，即 <span class="arithmatex">\(||x - (x+\delta)||_2\)</span>，而我们已经证明，乘以正交矩阵之后该距离没有变化，因此扰动的程度没有改变。</p>
<h3 id="1172">练习11.7.2<a class="headerlink" href="#1172" title="Permanent link">⚓︎</a></h3>
<p>尝试对函数<span class="arithmatex">\(f(\mathbf{x}) = 0.1 x_1^2 + 2 x_2^2\)</span>、以及它旋转45度后的函数即<span class="arithmatex">\(f(\mathbf{x}) = 0.1 (x_1 + x_2)^2 + 2 (x_1 - x_2)^2\)</span>使用AdaGrad算法。它的表现会不同吗？</p>
<p><strong>解答：</strong></p>
<p>我们首先对 <span class="arithmatex">\(f(x) = 0.1x_1^2 + 2x_2^2\)</span> 使用 AdaGrad 算法：</p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="k">def</span> <span class="nf">adagrad_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x2</span>
    <span class="n">s1</span> <span class="o">+=</span> <span class="n">g1</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">s2</span> <span class="o">+=</span> <span class="n">g2</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">x1</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s1</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">g1</span>
    <span class="n">x2</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s2</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">g2</span>
    <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span>

<span class="k">def</span> <span class="nf">f_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f_2d</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">train_2d</span><span class="p">(</span><span class="n">adagrad_2d</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 20, x1: -2.382563, x2: -0.158591
</code></pre></div>
<p><img alt="svg" src="../output_141_1.svg" /></p>
<p>接着对旋转后的函数 <span class="arithmatex">\(f(\mathbf{x}) = 0.1 (x_1 + x_2)^2 + 2 (x_1 - x_2)^2\)</span> 使用 AdaGrad 算法：</p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="k">def</span> <span class="nf">adagrad_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="mf">4.2</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="mf">3.8</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="mf">4.2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="mf">3.8</span> <span class="o">*</span> <span class="n">x1</span>
    <span class="n">s1</span> <span class="o">+=</span> <span class="n">g1</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">s2</span> <span class="o">+=</span> <span class="n">g2</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">x1</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s1</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">g1</span>
    <span class="n">x2</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s2</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">g2</span>
    <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span>

<span class="k">def</span> <span class="nf">f_2d_2</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f_2d</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">train_2d</span><span class="p">(</span><span class="n">adagrad_2d</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 20, x1: -3.180865, x2: -3.062780
</code></pre></div>
<p><img alt="svg" src="../output_143_1.svg" /></p>
<p>可以看到，两者的步长都是逐渐衰减的。</p>
<h3 id="1173">练习11.7.3<a class="headerlink" href="#1173" title="Permanent link">⚓︎</a></h3>
<p>证明<a href="https://en.wikipedia.org/wiki/Gershgorin_circle_theorem">格什戈林圆盘定理</a>，其中提到，矩阵<span class="arithmatex">\(\mathbf{M}\)</span>的特征值<span class="arithmatex">\(\lambda_i\)</span>在至少一个<span class="arithmatex">\(j\)</span>的选项中满足<span class="arithmatex">\(|\lambda_i - \mathbf{M}_{jj}| \leq \sum_{k \neq j} |\mathbf{M}_{jk}|\)</span>的要求。</p>
<p><strong>解答：</strong></p>
<p>我们假设 <span class="arithmatex">\(\zeta\)</span> 是 <span class="arithmatex">\(M\)</span> 对应特征值 <span class="arithmatex">\(\lambda_i\)</span> 的特征向量，即：</p>
<div class="arithmatex">\[ \lambda_i M = \zeta \lambda_i \]</div>
<p>将上式改写为线性方程组形式：</p>
<div class="arithmatex">\[
\begin{cases}
M_{11}x_1 + M_{12}x_2 + ... + M_{1n}x_n = \lambda_ix_1\\
M_{21}x_1 + M_{22}x_2 + ... + M_{2n}x_n = \lambda_ix_2\\
...\\
M_{n1}x_1 + M_{n2}x_2 + ... + M_{nn}x_n = \lambda_ix_n
\end{cases}
\]</div>
<p>我们假设：</p>
<div class="arithmatex">\[argmax |x_n| = j \]</div>
<p>则根据上式变化可得：</p>
<div class="arithmatex">\[(\lambda_i - M_{jj})x_j = M_{j1}x_1 + M_{j2}x_2 + ... + M_{jn}x_n\]</div>
<p>则</p>
<div class="arithmatex">\[|\lambda_i - M_{jj}||x_j| \leq |M_{j1}||x_1| + |M_{j2}||x_2| + ... + |M_{jn}||x_n| \]</div>
<div class="arithmatex">\[|\lambda_i - M_{jj}||x_j| \leq ({M_{j1}} + ... M_{jn})|x_j|\]</div>
<p>则证得：</p>
<div class="arithmatex">\[|\lambda_i - M_{jj}| \leq \sum_{k != j}|M_{jk}|\]</div>
<h3 id="1174">练习11.7.4<a class="headerlink" href="#1174" title="Permanent link">⚓︎</a></h3>
<p>关于对角线预处理矩阵<span class="arithmatex">\(\mathrm{diag}^{-\frac{1}{2}}(\mathbf{M}) \mathbf{M} \mathrm{diag}^{-\frac{1}{2}}(\mathbf{M})\)</span>的特征值，格什戈林的定理告诉了我们什么？</p>
<p><strong>解答：</strong></p>
<p>格什戈林定理可用于估计对角线预处理矩阵的特征值：</p>
<p>当我们需要求解矩阵 <span class="arithmatex">\(M\)</span> 的特征值和特征向量，即对：</p>
<div class="arithmatex">\[Mx = \lambda x\]</div>
<p>其中 x 是非零特征向量，<span class="arithmatex">\(\lambda\)</span> 是对应的特征值。对角线预处理矩阵 <span class="arithmatex">\(D\)</span> 通常是 <span class="arithmatex">\(M\)</span> 的对角线元素的逆构成的对角矩阵，即</p>
<div class="arithmatex">\[D = diag^{-\frac{1}{2}}(M)Mdiag^{-\frac{1}{2}}\]</div>
<p>则经过对角线预处理之后，我们需要求解的特征值问题为：</p>
<div class="arithmatex">\[Dz = \mu z\]</div>
<p>其中，<span class="arithmatex">\(z = diag^{-\frac{1}{2}}(x)\)</span>，<span class="arithmatex">\(\mu = \lambda\)</span>。</p>
<p>再根据格舍高林定理可得，对角线预处理后的矩阵 <span class="arithmatex">\(M\)</span> 的特征值 <span class="arithmatex">\(\mu\)</span> 落在以 <span class="arithmatex">\(M\)</span> 的对角线元素为中心、以各行非对角线元素绝对值之和为半径的圆盘内。换句话说，对角线预处理后的特征值分布在一些以对角线元素为中心的圆盘内。</p>
<h3 id="1175">练习11.7.5<a class="headerlink" href="#1175" title="Permanent link">⚓︎</a></h3>
<p>尝试对适当的深度网络使用AdaGrad算法，例如，6.6节中应用于Fashion-MNIST的深度网络。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_data_fashion_mnist</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate_accuracy_gpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># 设置为评估模式</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">device</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span><span class="o">.</span><span class="n">device</span>
    <span class="c1"># 正确预测的数量，总预测的数量</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="c1"># BERT微调所需的（之后将介绍）</span>
                <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">train_ch6</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;training on&#39;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">animator</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">],</span>
                            <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train loss&#39;</span><span class="p">,</span> <span class="s1">&#39;train acc&#39;</span><span class="p">,</span> <span class="s1">&#39;test acc&#39;</span><span class="p">])</span>
    <span class="n">timer</span><span class="p">,</span> <span class="n">num_batches</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Timer</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="c1"># 训练损失之和，训练准确率之和，样本数</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
            <span class="n">timer</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">l</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d2l</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
            <span class="n">train_l</span> <span class="o">=</span> <span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">train_acc</span> <span class="o">=</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_batches</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_batches</span><span class="p">,</span>
                             <span class="p">(</span><span class="n">train_l</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">evaluate_accuracy_gpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
        <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;loss </span><span class="si">{</span><span class="n">train_l</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, train acc </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;test acc </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">num_epochs</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">timer</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1"> examples/sec &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;on </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mi">20</span>
<span class="n">train_ch6</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">())</span>
</code></pre></div>
<p>接着，我们将 SGD 优化器替换为 AdaGrad 优化器：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">evaluate_accuracy_gpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span> <span class="c1">#@save</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># 设置为评估模式</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">device</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span><span class="o">.</span><span class="n">device</span>
    <span class="c1"># 正确预测的数量，总预测的数量</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="c1"># BERT微调所需的（之后将介绍）</span>
                <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1">#@save</span>
<span class="k">def</span> <span class="nf">train_ch6</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;training on&#39;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">animator</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">],</span>
                            <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train loss&#39;</span><span class="p">,</span> <span class="s1">&#39;train acc&#39;</span><span class="p">,</span> <span class="s1">&#39;test acc&#39;</span><span class="p">])</span>
    <span class="n">timer</span><span class="p">,</span> <span class="n">num_batches</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Timer</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="c1"># 训练损失之和，训练准确率之和，样本数</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
            <span class="n">timer</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">l</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d2l</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
            <span class="n">train_l</span> <span class="o">=</span> <span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">train_acc</span> <span class="o">=</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_batches</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_batches</span><span class="p">,</span>
                             <span class="p">(</span><span class="n">train_l</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">evaluate_accuracy_gpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
        <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;loss </span><span class="si">{</span><span class="n">train_l</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, train acc </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;test acc </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">num_epochs</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">timer</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1"> examples/sec &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;on </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mi">20</span>
<span class="n">train_ch6</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">())</span>
</code></pre></div>
<p>可以看出，使用 AdaGrad 算法在一开始 loss 下降得更陡峭、迅速，但随后 loss 下降速度开始衰减，显著慢于使用 SGD 算法，以及 20 个 epoch 迭代之后我们对比发现，虽然 AdaGrad 算法在前3个 epoch 就实现了 loss 的骤降，但最终并没能实现稳定的收敛。</p>
<h3 id="1176">练习11.7.6<a class="headerlink" href="#1176" title="Permanent link">⚓︎</a></h3>
<p>要如何修改AdaGrad算法，才能使其在学习率方面的衰减不那么剧烈？</p>
<p><strong>解答：</strong></p>
<p>AdaGrad 算法之所以在学习率方面的衰减会越发激进，是因为 AdaGrad 算法会累积历史梯度，造成后续学习率很小，权值无法得到有效更新。</p>
<p>为改进这一问题，有学者提出了 RMSProp 算法，该算法增加了一个衰减系数来控制历史信息的获取，即我们在下一章将要学习的算法。</p>
<h2 id="118-rmsprop">11.8 RMSProp算法<a class="headerlink" href="#118-rmsprop" title="Permanent link">⚓︎</a></h2>
<h3 id="1181">练习11.8.1<a class="headerlink" href="#1181" title="Permanent link">⚓︎</a></h3>
<p>如果我们设置<span class="arithmatex">\(\gamma = 1\)</span>，实验会发生什么？为什么？</p>
<p><strong>解答：</strong></p>
<p>如果我们设置 <span class="arithmatex">\(\gamma\)</span> 为1，则状态矢量 <span class="arithmatex">\(s_t\)</span> 的更新函数变为：</p>
<div class="arithmatex">\[s_t \leftarrow s_{t-1} = s_0 = 0\]</div>
<p>也就是状态矢量将保持不变为0，则自变量更新函数变为：</p>
<div class="arithmatex">\[x_t \leftarrow x_{t-1} - \frac{\eta}{\sqrt{\epsilon}}\odot g(t)\]</div>
<p>因此，步长将不会发生改变，将变成确定步长优化。</p>
<h3 id="1182">练习11.8.2<a class="headerlink" href="#1182" title="Permanent link">⚓︎</a></h3>
<p>旋转优化问题以最小化<span class="arithmatex">\(f(\mathbf{x}) = 0.1 (x_1 + x_2)^2 + 2 (x_1 - x_2)^2\)</span>。收敛会发生什么？</p>
<p><strong>解答：</strong></p>
<p>我们首先观察原优化问题：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="k">def</span> <span class="nf">rmsprop_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">):</span>
    <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="mf">1e-6</span>
    <span class="n">s1</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">*</span> <span class="n">g1</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">s2</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">*</span> <span class="n">g2</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">x1</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s1</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">g1</span>
    <span class="n">x2</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s2</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">g2</span>
    <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span>

<span class="k">def</span> <span class="nf">f_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">eta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.9</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f_2d</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">train_2d</span><span class="p">(</span><span class="n">rmsprop_2d</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 20, x1: -0.010599, x2: 0.000000
</code></pre></div>
<p><img alt="svg" src="../output_169_1.svg" /></p>
<p>再关注旋转后的优化问题</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">rmsprop_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">):</span>
    <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">4.2</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="mf">3.8</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="mf">4.2</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="mf">3.8</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="mf">1e-6</span>
    <span class="n">s1</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">*</span> <span class="n">g1</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">s2</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">*</span> <span class="n">g2</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">x1</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s1</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">g1</span>
    <span class="n">x2</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s2</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">g2</span>
    <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span>

<span class="k">def</span> <span class="nf">f_2d</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">eta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.9</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">show_trace_2d</span><span class="p">(</span><span class="n">f_2d</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">train_2d</span><span class="p">(</span><span class="n">rmsprop_2d</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>epoch 20, x1: -0.927150, x2: -0.914018
</code></pre></div>
<p><img alt="svg" src="../output_171_1.svg" /></p>
<p>可见，旋转之后的优化问题收敛效果显著变差。</p>
<h3 id="1183">练习11.8.3<a class="headerlink" href="#1183" title="Permanent link">⚓︎</a></h3>
<p>尝试在真正的机器学习问题上应用RMSProp算法会发生什么，例如在Fashion-MNIST上的训练。试验不同的取值来调整学习率。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_data_fashion_mnist</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate_accuracy_gpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># 设置为评估模式</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">device</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span><span class="o">.</span><span class="n">device</span>
    <span class="c1"># 正确预测的数量，总预测的数量</span>
    <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="c1"># BERT微调所需的（之后将介绍）</span>
                <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">d2l</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">train_ch6</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span><span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;training on&#39;</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.9</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">animator</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">],</span>
                            <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train loss&#39;</span><span class="p">,</span> <span class="s1">&#39;train acc&#39;</span><span class="p">,</span> <span class="s1">&#39;test acc&#39;</span><span class="p">])</span>
    <span class="n">timer</span><span class="p">,</span> <span class="n">num_batches</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Timer</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="c1"># 训练损失之和，训练准确率之和，样本数</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
            <span class="n">timer</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">l</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d2l</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
            <span class="n">train_l</span> <span class="o">=</span> <span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">train_acc</span> <span class="o">=</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_batches</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_batches</span><span class="p">,</span>
                             <span class="p">(</span><span class="n">train_l</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">evaluate_accuracy_gpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
        <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;loss </span><span class="si">{</span><span class="n">train_l</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, train acc </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;test acc </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">num_epochs</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">timer</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1"> examples/sec &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;on </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz


100%|██████████| 26421880/26421880 [00:01&lt;00:00, 19463394.71it/s]


Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz


100%|██████████| 29515/29515 [00:00&lt;00:00, 301472.56it/s]


Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz


100%|██████████| 4422102/4422102 [00:00&lt;00:00, 5532661.80it/s]


Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz


100%|██████████| 5148/5148 [00:00&lt;00:00, 14860479.69it/s]

Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw




/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">train_ch6</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">())</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

&lt;ipython-input-27-bc163efaccea&gt; in &lt;cell line: 2&gt;()
      1 num_epochs = 10
----&gt; 2 train_ch6(net, train_iter, test_iter, num_epochs, d2l.try_gpu())


&lt;ipython-input-26-5fba167c60a8&gt; in train_ch6(net, train_iter, test_iter, num_epochs, device)
     59             y_hat = net(X)
     60             l = loss(y_hat, y)
---&gt; 61             l.backward()
     62             optimizer.step()
     63             with torch.no_grad():


/usr/local/lib/python3.10/dist-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)
    485                 inputs=inputs,
    486             )
--&gt; 487         torch.autograd.backward(
    488             self, gradient, retain_graph, create_graph, inputs=inputs
    489         )


/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    198     # some Python versions print out the first line of a multi-line function
    199     # calls in the traceback and some print out the last line
--&gt; 200     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    201         tensors, grad_tensors_, retain_graph, create_graph, inputs,
    202         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass


KeyboardInterrupt:
</code></pre></div>
<p><img alt="svg" src="../output_176_1.svg" /></p>
<h3 id="1184">练习11.8.4<a class="headerlink" href="#1184" title="Permanent link">⚓︎</a></h3>
<p>随着优化的进展，需要调整<span class="arithmatex">\(\gamma\)</span>吗？RMSProp算法对此的敏感程度如何？</p>
<p><strong>解答：</strong></p>
<p>RMSProp 算法对 <span class="arithmatex">\(\gamma\)</span> 是比较敏感的，其取值对最终优化结果有较大影响；例如，在上一题应用 RMSProp 算法来优化 Mnist 数据集上的分类问题时，我们会发现使用默认的 <span class="arithmatex">\(\gamma = 0.99\)</span> 优化效果很不好，loss 几乎不下降；设置 <span class="arithmatex">\(\gamma = 0.9\)</span> 时，loss 得到了快速的下降。</p>
<p>由于 <span class="arithmatex">\(\gamma\)</span> 决定了在调整每坐标比例时历史记录的时长，当其值越大，步长衰减的速度就越慢；因此，我们应该在优化的初期设置较大的 <span class="arithmatex">\(\gamma\)</span> 值，在优化的后期设置较小的 <span class="arithmatex">\(\gamma\)</span> 值。</p>
<h2 id="119-adadelta">11.9 Adadelta<a class="headerlink" href="#119-adadelta" title="Permanent link">⚓︎</a></h2>
<h3 id="1191">练习11.9.1<a class="headerlink" href="#1191" title="Permanent link">⚓︎</a></h3>
<p>调整<span class="arithmatex">\(\rho\)</span>的值，会发生什么？</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">init_adadelta_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">):</span>
    <span class="n">s_w</span><span class="p">,</span> <span class="n">s_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">delta_w</span><span class="p">,</span> <span class="n">delta_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">s_w</span><span class="p">,</span> <span class="n">delta_w</span><span class="p">),</span> <span class="p">(</span><span class="n">s_b</span><span class="p">,</span> <span class="n">delta_b</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">adadelta</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="n">rho</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;rho&#39;</span><span class="p">],</span> <span class="mf">1e-5</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># In-placeupdatesvia[:]</span>
            <span class="n">s</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">s</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">delta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span> <span class="o">+</span> <span class="n">eps</span><span class="p">))</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">p</span><span class="p">[:]</span> <span class="o">-=</span> <span class="n">g</span>
            <span class="n">delta</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adadelta</span><span class="p">,</span>
        <span class="n">init_adadelta_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
        <span class="p">{</span><span class="s1">&#39;rho&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span>
        <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>

<span class="c1"># 直接调用torch的api</span>
<span class="c1"># trainer = torch.optim.Adadelta</span>
<span class="c1"># d2l.train_concise_ch11(trainer, {&#39;rho&#39;: 0.5}, data_iter)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.246, 0.277 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_184_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adadelta</span><span class="p">,</span>
        <span class="n">init_adadelta_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
        <span class="p">{</span><span class="s1">&#39;rho&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">},</span>
        <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.243, 0.190 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_185_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adadelta</span><span class="p">,</span>
        <span class="n">init_adadelta_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
        <span class="p">{</span><span class="s1">&#39;rho&#39;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">},</span>
        <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.245, 0.196 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_186_1.svg" /></p>
<ul>
<li>在这个数据集上参数<span class="arithmatex">\(\rho\)</span>的影响不大，loss均收敛。</li>
</ul>
<p><span class="arithmatex">\(\rho\)</span> 实则控制了优化时当前梯度和历史梯度的比例，当 <span class="arithmatex">\(\rho\)</span> 值越大，调整后梯度会更多地倾向于历史梯度；当 <span class="arithmatex">\(\rho\)</span> 值越小，调整后梯度会更多地倾向于当前梯度。</p>
<p>事实上，当 <span class="arithmatex">\(\rho\)</span> 越大，变化量衰减的速度就越慢；当 <span class="arithmatex">\(\rho\)</span> 越小，变化率衰减的速度就越快。</p>
<h3 id="1192">练习11.9.2<a class="headerlink" href="#1192" title="Permanent link">⚓︎</a></h3>
<p>展示如何在不使用<span class="arithmatex">\(\mathbf{g}_t'\)</span>的情况下实现算法。为什么这是个好主意？</p>
<p><strong>解答：</strong></p>
<p>暂时没做出来，不使用g的话，梯度无法更新，</p>
<h3 id="1193">练习11.9.3<a class="headerlink" href="#1193" title="Permanent link">⚓︎</a></h3>
<p>Adadelta真的是学习率为0吗？能找到Adadelta无法解决的优化问题吗？</p>
<p><strong>解答：</strong></p>
<ul>
<li>Adadelta算法中的学习率并不是固定为0。对于Adadelta算法，学习率是动态调整的，并且不需要手动设置一个固定的学习率值。</li>
</ul>
<p>Adadelta算法维护一个状态变量来存储这个指数移动平均值，并使用该变量来计算每个参数更新的学习率。</p>
<p>Adadelta 不能解决的优化问题包括：</p>
<ol>
<li>
<p>高度非凸问题:
Adadelta是基于梯度信息的自适应学习率算法。在高度非凸的优化问题中，梯度可能会变化非常剧烈，导致Adadelta难以适应这种剧烈变化。相比之下，一些更复杂的优化算法，如具有动量的随机梯度下降（SGD with momentum）或Adam可能能更好地处理这种情况。</p>
</li>
<li>
<p>离散优化问题:
Adadelta假设参数的连续更新，而在离散优化问题中，参数可能只能取离散的值。这种情况下，Adadelta可能不是最适合的选择。在离散优化问题中，特定的优化算法，如遗传算法或模拟退火等可能更适合。</p>
</li>
<li>
<p>奇异性质的问题:
一些优化问题可能具有奇异性质，即梯度在某些点上可能不存在或变得非常大。这种情况下，Adadelta可能会遇到数值稳定性的问题，因为它涉及梯度的平方根。其他一些优化算法可能会采取特殊的处理方式来处理奇异点。</p>
</li>
</ol>
<h3 id="1194">练习11.9.4<a class="headerlink" href="#1194" title="Permanent link">⚓︎</a></h3>
<p>将Adadelta的收敛行为与AdaGrad和RMSProp进行比较。</p>
<p><strong>解答：</strong></p>
<ul>
<li>
<p>AdaGrad 会自适应地调整学习率，使稀疏特征对应的参数具有较大的学习率，但是可能会在后期过早停止学习，因为梯度的平方和随时间增加，导致学习率变得很小，减缓了参数更新的速度；</p>
</li>
<li>
<p>而RMSProp 就是针对这一问题做出了优化，引入指数加权移动平均来平滑梯度的历史平方和，减缓了学习率的衰减，对学习率的适应性更好，能够更稳定地收敛到最优解；</p>
</li>
<li>
<p>AdaDelta 则是在 RMSProp 上做出进一步改进，避免了手动设置学习率，通常能更稳定地收敛到最优解，而且避免了手动调整学习率可能导致的参数不正确带来的不收敛。</p>
</li>
</ul>
<h2 id="1110-adam">11.10 Adam算法<a class="headerlink" href="#1110-adam" title="Permanent link">⚓︎</a></h2>
<h3 id="11101">练习11.10.1<a class="headerlink" href="#11101" title="Permanent link">⚓︎</a></h3>
<p>调节学习率，观察并分析实验结果。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>


<span class="k">def</span> <span class="nf">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">):</span>
    <span class="n">v_w</span><span class="p">,</span> <span class="n">v_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">s_w</span><span class="p">,</span> <span class="n">s_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">v_w</span><span class="p">,</span> <span class="n">s_w</span><span class="p">),</span> <span class="p">(</span><span class="n">v_b</span><span class="p">,</span> <span class="n">s_b</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">1e-6</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">v</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">s</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">s</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">v_bias_corr</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
            <span class="n">s_bias_corr</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
            <span class="n">p</span><span class="p">[:]</span> <span class="o">-=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">v_bias_corr</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s_bias_corr</span><span class="p">)</span>
                                                       <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adam</span><span class="p">,</span> <span class="n">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
               <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.244, 0.196 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_200_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adam</span><span class="p">,</span> <span class="n">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
               <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.250, 0.261 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_201_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adam</span><span class="p">,</span> <span class="n">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
               <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.265, 0.193 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_202_1.svg" /></p>
<h3 id="11102">练习11.10.2<a class="headerlink" href="#11102" title="Permanent link">⚓︎</a></h3>
<p>试着重写动量和二次矩更新，从而使其不需要偏差校正</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">init_adam_states_update</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">):</span>
    <span class="n">v_w</span><span class="p">,</span> <span class="n">v_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">s_w</span><span class="p">,</span> <span class="n">s_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">v_w</span><span class="p">,</span> <span class="n">s_w</span><span class="p">),</span> <span class="p">(</span><span class="n">v_b</span><span class="p">,</span> <span class="n">s_b</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">adam_update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">1e-6</span>
    <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># 先增加 t 的值</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">v</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">s</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">s</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">v_bias_corr</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>  <span class="c1"># 去除偏差校正</span>
            <span class="n">s_bias_corr</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>  <span class="c1"># 去除偏差校正</span>
            <span class="n">p</span><span class="p">[:]</span> <span class="o">-=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">v_bias_corr</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s_bias_corr</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
               <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</code></pre></div>
<p><img alt="svg" src="../output_206_0.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
               <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.303, 0.225 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_207_1.svg" /></p>
<h3 id="11103">练习11.10.3<a class="headerlink" href="#11103" title="Permanent link">⚓︎</a></h3>
<p>收敛时为什么需要降低学习率<span class="arithmatex">\(\eta\)</span>?</p>
<p><strong>解答：</strong></p>
<p>在Adam算法中，有两个关键的超参数：学习率<span class="arithmatex">\(\eta\)</span>和指数衰减率<span class="arithmatex">\(\beta_1\)</span>、<span class="arithmatex">\(\beta_2\)</span>。</p>
<p>当Adam算法收敛时，降低学习率<span class="arithmatex">\(\eta\)</span>的原因如下：</p>
<ol>
<li>
<p><strong>动量的影响</strong>：
   Adam算法利用动量来加速梯度下降过程，动量可以帮助算法跳出局部极小值并加速在平坦区域的移动。然而，在接近收敛时，我们希望算法逐渐减缓速度，以避免在极小值附近来回震荡。</p>
</li>
<li>
<p><strong>过大的学习率会导致震荡</strong>：
   如果学习率<span class="arithmatex">\(\eta\)</span>过大，那么在收敛阶段，更新步长可能会过大，使得算法在极小值附近来回震荡，而无法稳定地收敛。</p>
</li>
<li>
<p><strong>稳定性和精度</strong>：
   当接近最优解时，我们希望算法收敛得更加精确和稳定。通过降低学习率，我们可以确保算法在接近最优解时，以更小的步长进行微调，从而更加精确地找到最优解。</p>
</li>
<li>
<p><strong>避免“振荡”</strong>：
   在接近最优解时，如果学习率<span class="arithmatex">\(\eta\)</span>过大，可能会导致优化过程在最优解附近来回震荡，而无法稳定地收敛。通过降低学习率，可以减缓更新的速度，避免这种振荡现象。</p>
</li>
</ol>
<p>所以，当Adam算法接近收敛时，逐步降低学习率<span class="arithmatex">\(\eta\)</span>是为了保证优化过程的稳定性和精确性，避免在最优解附近震荡，并最终找到一个满足要求的最优解。</p>
<h3 id="11104">练习11.10.4<a class="headerlink" href="#11104" title="Permanent link">⚓︎</a></h3>
<p>尝试构造一个使用Adam算法会发散而Yogi会收敛的例子。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">):</span>
    <span class="n">v_w</span><span class="p">,</span> <span class="n">v_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">s_w</span><span class="p">,</span> <span class="n">s_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">feature_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">v_w</span><span class="p">,</span> <span class="n">s_w</span><span class="p">),</span> <span class="p">(</span><span class="n">v_b</span><span class="p">,</span> <span class="n">s_b</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">adam</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">1e-1</span> <span class="c1"># adam的eps调大</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">v</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">s</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">s</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">v_bias_corr</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
            <span class="n">s_bias_corr</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
            <span class="n">p</span><span class="p">[:]</span> <span class="o">-=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">v_bias_corr</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s_bias_corr</span><span class="p">)</span><span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">yogi</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">eps</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">1e-6</span> <span class="c1">#yogi的eps调小</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">v</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">s</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">s</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">v_bias_corr</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
            <span class="n">s_bias_corr</span> <span class="o">=</span> <span class="n">s</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">])</span>
            <span class="n">p</span><span class="p">[:]</span> <span class="o">-=</span> <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">v_bias_corr</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s_bias_corr</span><span class="p">)</span><span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">hyperparams</span><span class="p">[</span><span class="s1">&#39;t&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">adam</span><span class="p">,</span> <span class="n">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.286, 0.254 sec/epoch





([0.02502727508544922,
  0.05432486534118652,
  0.07659363746643066,
  0.09993433952331543,
  0.12483882904052734,
  0.1624300479888916,
  0.2031557559967041,
  0.23590397834777832,
  0.2621176242828369,
  0.29068803787231445,
  0.31475114822387695,
  0.33946847915649414,
  0.3629024028778076,
  0.4381234645843506,
  0.5079770088195801],
 [0.2802133682370186,
  0.24929887652397156,
  0.25547001020113624,
  0.2572503798007965,
  0.25765814197063447,
  0.25446937972307204,
  0.24969262937704723,
  0.2474356138308843,
  0.2543005377848943,
  0.26428999155759814,
  0.26891206375757853,
  0.2554288481871287,
  0.25635744649171827,
  0.27924392398198444,
  0.28562702345848084])
</code></pre></div>
<p><img alt="svg" src="../output_215_2.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">get_data_ch11</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">train_ch11</span><span class="p">(</span><span class="n">yogi</span><span class="p">,</span> <span class="n">init_adam_states</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">),</span>
               <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">data_iter</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">);</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>loss: 0.245, 0.198 sec/epoch
</code></pre></div>
<p><img alt="svg" src="../output_216_1.svg" /></p>
<h2 id="1111_1">11.11 学习率调度器<a class="headerlink" href="#1111_1" title="Permanent link">⚓︎</a></h2>
<h3 id="11111">练习11.11.1<a class="headerlink" href="#11111" title="Permanent link">⚓︎</a></h3>
<p>试验给定固定学习率的优化行为。这种情况下可以获得的最佳模型是什么？</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">lr_scheduler</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>


<span class="k">def</span> <span class="nf">net_fn</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">try_gpu</span><span class="p">()</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">load_data_fashion_mnist</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="c1"># 代码几乎与d2l.train_ch6定义在卷积神经网络一章LeNet一节中的相同</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
          <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">animator</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">],</span>
                            <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;train loss&#39;</span><span class="p">,</span> <span class="s1">&#39;train acc&#39;</span><span class="p">,</span> <span class="s1">&#39;test acc&#39;</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># train_loss,train_acc,num_examples</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
            <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">metric</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">l</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d2l</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">train_acc</span> <span class="o">=</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="n">i</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">),</span>
                             <span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">evaluate_accuracy_gpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
        <span class="n">animator</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">scheduler</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">scheduler</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">==</span> <span class="n">lr_scheduler</span><span class="o">.</span><span class="vm">__name__</span><span class="p">:</span>
                <span class="c1"># UsingPyTorchIn-Builtscheduler</span>
                <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Usingcustomdefinedscheduler</span>
                <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                    <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;train loss </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, train acc </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">, &#39;</span>
          <span class="sa">f</span><span class="s1">&#39;test acc </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">lr</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">30</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">net_fn</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

&lt;ipython-input-64-7c88355f57f7&gt; in &lt;cell line: 71&gt;()
     69 net = net_fn()
     70 trainer = torch.optim.SGD(net.parameters(), lr=lr)
---&gt; 71 train(net, train_iter, test_iter, num_epochs, loss, trainer, device)


&lt;ipython-input-64-7c88355f57f7&gt; in train(net, train_iter, test_iter, num_epochs, loss, trainer, device, scheduler)
     39             trainer.zero_grad()
     40             X, y = X.to(device), y.to(device)
---&gt; 41             y_hat = net(X)
     42             l = loss(y_hat, y)
     43             l.backward()


/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []


/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py in forward(self, input)
    215     def forward(self, input):
    216         for module in self:
--&gt; 217             input = module(input)
    218         return input
    219


/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []


/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py in forward(self, input)
    164 
    165     def forward(self, input: Tensor):
--&gt; 166         return F.max_pool2d(input, self.kernel_size, self.stride,
    167                             self.padding, self.dilation, ceil_mode=self.ceil_mode,
    168                             return_indices=self.return_indices)


/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py in fn(*args, **kwargs)
    482             return if_true(*args, **kwargs)
    483         else:
--&gt; 484             return if_false(*args, **kwargs)
    485 
    486     if if_true.__doc__ is None and if_false.__doc__ is not None:


/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in _max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)
    780     if stride is None:
    781         stride = torch.jit.annotate(List[int], [])
--&gt; 782     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
    783 
    784


KeyboardInterrupt:
</code></pre></div>
<p><img alt="svg" src="../output_220_1.svg" /></p>
<div class="highlight"><pre><span></span><code><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;learning rate is now </span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">SquareRootScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_update</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="nb">pow</span><span class="p">(</span><span class="n">num_update</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CosineScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_update</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">final_lr</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">warmup_begin_lr</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_lr_orig</span> <span class="o">=</span> <span class="n">base_lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_update</span> <span class="o">=</span> <span class="n">max_update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_lr</span> <span class="o">=</span> <span class="n">final_lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_begin_lr</span> <span class="o">=</span> <span class="n">warmup_begin_lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_update</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span>

    <span class="k">def</span> <span class="nf">get_warmup_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="n">increase</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lr_orig</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_begin_lr</span><span class="p">)</span> \
                       <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_begin_lr</span> <span class="o">+</span> <span class="n">increase</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_warmup_lr</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_update</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_lr</span> <span class="o">+</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">base_lr_orig</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_lr</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span>
                <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_steps</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineScheduler</span><span class="p">(</span><span class="n">max_update</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">final_lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span> <span class="p">[</span><span class="n">scheduler</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)])</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">SquareRootScheduler</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span> <span class="p">[</span><span class="n">scheduler</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">net</span> <span class="o">=</span> <span class="n">net_fn</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
<span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span>
</code></pre></div>
<h3 id="11112">练习11.11.2<a class="headerlink" href="#11112" title="Permanent link">⚓︎</a></h3>
<p>如果改变学习率下降的指数，收敛性会如何改变？在实验中方便起见，使用<code>PolyScheduler</code>。</p>
<p><strong>解答：</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">PolynomialLR</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">net_fn</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
<span class="c1"># 定义PolyScheduler并设置参数</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">PolynomialLR</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">total_iters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>
</code></pre></div>
<h3 id="11113">练习11.11.3<a class="headerlink" href="#11113" title="Permanent link">⚓︎</a></h3>
<p>将余弦调度器应用于大型计算机视觉问题，例如训练ImageNet数据集。与其他调度器相比，它如何影响性能？</p>
<p><strong>解答：</strong></p>
<p>与其他调度器相比，余弦调度器在性能方面有一些独特的影响。</p>
<ol>
<li>
<p>初始学习率设置：余弦调度器要求设置一个初始学习率。通常情况下，较大的学习率可以加快模型的收敛速度，但同时也可能导致训练过程中的不稳定性。因此，选择一个合适的初始学习率对于获得较好的性能至关重要。</p>
</li>
<li>
<p>学习率下降：余弦调度器通过在训练过程中逐渐降低学习率来优化模型的训练。它使用了余弦函数的形式来调整学习率，使其在训练的不同阶段按照一种平滑的方式进行下降。这种方式相对于其他调度器（如固定学习率或按照一定步长进行衰减的调度器）可以更好地平衡模型的收敛速度和稳定性。</p>
</li>
<li>
<p>阶段划分：余弦调度器将整个训练过程划分为多个阶段，每个阶段具有不同的学习率。这些阶段的划分可以根据训练数据的大小和复杂性进行调整。通常情况下，初始阶段使用较大的学习率以加快模型的收敛，后续阶段逐渐降低学习率以确保模型更好地拟合数据。</p>
</li>
<li>
<p>平滑下降：余弦调度器的学习率下降过程相对平滑，不像其他调度器那样存在突然的学习率变化。这种平滑的下降可以帮助模型避免陷入局部极小值，并且在优化过程中更容易跳出局部极小值并找到全局最优解。</p>
</li>
</ol>
<p>余弦调度器在大型计算机视觉问题中的应用可以通过逐渐降低学习率、平滑下降以及合理的阶段划分来提升性能。它可以帮助模型更好地收敛，避免过拟合，并在训练过程中更好地平衡收敛速度和稳定性。</p>
<h3 id="11114">练习11.11.4<a class="headerlink" href="#11114" title="Permanent link">⚓︎</a></h3>
<p>预热应该持续多长时间？</p>
<p><strong>解答：</strong></p>
<p>预热时间的选择可以基于以下几个因素进行考虑：</p>
<ul>
<li>
<p>模型的复杂性：复杂的模型可能需要更长的预热时间来帮助模型更好地收敛。较简单的模型可能在较短的预热时间内就能达到较好的性能。</p>
</li>
<li>
<p>数据集的大小：大型数据集通常需要更长的预热时间，因为模型需要更多的迭代次数来适应数据集的复杂性。相反，小型数据集可能在较短的预热时间内就能够达到较好的性能。</p>
</li>
<li>
<p>训练时间和计算资源：预热时间的选择还应考虑到可用的训练时间和计算资源。如果时间和资源有限，可能需要选择较短的预热时间来加快模型的训练速度。</p>
</li>
</ul>
<p>一种常见的做法是通过观察模型的训练曲线和验证集的性能来确定预热时间。可以尝试不同的预热时间，并观察模型在训练过程中的性能变化。如果模型在预热阶段表现不稳定或训练损失仍然很高，可能需要增加预热时间。相反，如果模型在较短的预热时间内就能达到较好的性能，可以考虑减少预热时间以加快训练速度。</p>
<div class="highlight"><pre><span></span><code><span class="n">scheduler</span> <span class="o">=</span> <span class="n">CosineScheduler</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">final_lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span> <span class="p">[</span><span class="n">scheduler</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)])</span>
</code></pre></div>
<p><img alt="svg" src="../output_235_0.svg" /></p>
<h3 id="11115">练习11.11.5<a class="headerlink" href="#11115" title="Permanent link">⚓︎</a></h3>
<p>可以试着把优化和采样联系起来吗？首先，在随机梯度朗之万动力学上使用 :cite:<code>Welling.Teh.2011</code>的结果。</p>
<p><strong>解答：</strong></p>
<ul>
<li>
<p>随机梯度下降（SGD）：在优化神经网络中，随机梯度下降是一种常用的优化算法。它通过使用每个训练样本的随机梯度来更新模型的参数。SGD的主要思想是沿着梯度方向更新参数，以最小化损失函数。</p>
</li>
<li>
<p>朗之万动力学（Langevin Dynamics）：朗之万动力学是一种物理学中的模型，用于描述粒子在势能场中的随机运动。将朗之万动力学引入优化中，可以通过在参数更新中添加随机扰动来模拟参数的随机变化。这种随机性可以帮助模型跳出局部极小值，并在优化过程中探索更广阔的参数空间。</p>
</li>
</ul>
<p>结合SGD和朗之万动力学的特点，于是有了SGLD算法</p>
<p><strong>SGLD算法</strong>：SGLD算法结合了随机梯度下降和朗之万动力学。它在每次参数更新时，使用随机梯度下降的步骤进行参数更新，并添加一个朗之万动力学的随机扰动项。这个随机扰动项是由高斯噪声和学习率控制的。通过这种方式，SGLD算法可以在优化过程中进行采样，从而估计参数的后验分布。</p>

  <hr>
<div class="md-source-file">
  <small>
    
      最后更新:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 25, 2023</span>
      
        <br>
        创建日期:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 25, 2023</span>
      
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../../ch10/ch10/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 注意力机制">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                注意力机制
              </div>
            </div>
          </a>
        
        
          
          <a href="../../ch12/ch12/" class="md-footer__link md-footer__link--next" aria-label="下一页: 第十二章 计算性能">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                第十二章 计算性能
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 Ean Yang
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/YQisme" target="_blank" rel="noopener" title="github主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://space.bilibili.com/244185393?spm_id_from=333.788.0.0" target="_blank" rel="noopener" title="b站主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M488.6 104.1c16.7 18.1 24.4 39.7 23.3 65.7v202.4c-.4 26.4-9.2 48.1-26.5 65.1-17.2 17-39.1 25.9-65.5 26.7H92.02c-26.45-.8-48.21-9.8-65.28-27.2C9.682 419.4.767 396.5 0 368.2V169.8c.767-26 9.682-47.6 26.74-65.7C43.81 87.75 65.57 78.77 92.02 78h29.38L96.05 52.19c-5.75-5.73-8.63-13-8.63-21.79 0-8.8 2.88-16.06 8.63-21.797C101.8 2.868 109.1 0 117.9 0s16.1 2.868 21.9 8.603L213.1 78h88l74.5-69.397C381.7 2.868 389.2 0 398 0c8.8 0 16.1 2.868 21.9 8.603 5.7 5.737 8.6 12.997 8.6 21.797 0 8.79-2.9 16.06-8.6 21.79L394.6 78h29.3c26.4.77 48 9.75 64.7 26.1zm-38.8 69.7c-.4-9.6-3.7-17.4-10.7-23.5-5.2-6.1-14-9.4-22.7-9.8H96.05c-9.59.4-17.45 3.7-23.58 9.8-6.14 6.1-9.4 13.9-9.78 23.5v194.4c0 9.2 3.26 17 9.78 23.5s14.38 9.8 23.58 9.8H416.4c9.2 0 17-3.3 23.3-9.8 6.3-6.5 9.7-14.3 10.1-23.5V173.8zm-264.3 42.7c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.2 6.3-14 9.5-23.6 9.5-9.6 0-17.5-3.2-23.6-9.5-6.1-6.3-9.4-14-9.8-23.2v-33.3c.4-9.1 3.8-16.9 10.1-23.2 6.3-6.3 13.2-9.6 23.3-10 9.2.4 17 3.7 23.3 10zm191.5 0c6.3 6.3 9.7 14.1 10.1 23.2V273c-.4 9.2-3.7 16.9-9.8 23.2-6.1 6.3-14 9.5-23.6 9.5-9.6 0-17.4-3.2-23.6-9.5-7-6.3-9.4-14-9.7-23.2v-33.3c.3-9.1 3.7-16.9 10-23.2 6.3-6.3 14.1-9.6 23.3-10 9.2.4 17 3.7 23.3 10z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://eanyang7.com" target="_blank" rel="noopener" title="个人主页" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M112 48a48 48 0 1 1 96 0 48 48 0 1 1-96 0zm40 304v128c0 17.7-14.3 32-32 32s-32-14.3-32-32V256.9l-28.6 47.6c-9.1 15.1-28.8 20-43.9 10.9s-20-28.8-10.9-43.9l58.3-97c17.4-28.9 48.6-46.6 82.3-46.6h29.7c33.7 0 64.9 17.7 82.3 46.6l58.3 97c9.1 15.1 4.2 34.8-10.9 43.9s-34.8 4.2-43.9-10.9L232 256.9V480c0 17.7-14.3 32-32 32s-32-14.3-32-32V352h-16z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.instant", "navigation.instant.progress", "navigation.tracking", "navigation.tabs", "navigation.prune", "navigation.top", "toc.follow", "header.autohide", "navigation.footer", "search.suggest", "search.highlight", "search.share", "content.action.edit", "content.action.view", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.6c14ae12.min.js"></script>
      
    
  </body>
</html>